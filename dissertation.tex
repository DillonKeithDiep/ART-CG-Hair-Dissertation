% The document class supplies options to control rendering of some standard
% features in the result.  The goal is for uniform style, so some attention 
% to detail is *vital* with all fields.  Each field (i.e., text inside the
% curly braces below, so the MEng text inside {MEng} for instance) should 
% take into account the following:
%
% - author name       should be formatted as "FirstName LastName"
%   (not "Initial LastName" for example),
% - supervisor name   should be formatted as "Title FirstName LastName"
%   (where Title is "Dr." or "Prof." for example),
% - degree programme  should be "BSc", "MEng", "MSci", "MSc" or "PhD",
% - dissertation title should be correctly capitalised (plus you can have
%   an optional sub-title if appropriate, or leave this field blank),
% - dissertation type should be formatted as one of the following:
%   * for the MEng degree programme either "enterprise" or "research" to
%     reflect the stream,
%   * for the MSc  degree programme "$X/Y/Z$" for a project deemed to be
%     X%, Y% and Z% of type I, II and III.
% - year              should be formatted as a 4-digit year of submission
%   (so 2014 rather than the accademic year, say 2013/14 say).

\documentclass[ % the name of the author
                    author={Dillon Keith Diep},
                % the name of the supervisor
                supervisor={Dr. Carl Henrik Ek},
                % the degree programme
                    degree={MEng},
                % the dissertation    title (which cannot be blank)
                     title={Assisted Content Generation for 3D Hair Geometry},
                % the dissertation subtitle (which can    be blank)
                  subtitle={[INCOMPLETE DRAFT, CONTAINS NOTES FROM RESEARCH]},
                % the dissertation     type
                      type={Research},
                % the year of submission
                      year={2014} ]{dissertation}

\begin{document}

% =============================================================================

% This section simply introduces the structural guidelines.  It can clearly
% be deleted (or commented out) if you use the file as a template for your
% own dissertation: everything following it is in the correct order to use 
% as is.

% =============================================================================

% This macro creates the standard UoB title page by using information drawn
% from the document class (meaning it is vital you select the correct degree 
% title and so on).

\maketitle

% After the title page (which is a special case in that it is not numbered)
% comes the front matter or preliminaries; this macro signals the start of
% such content, meaning the pages are numbered with Roman numerals.

\frontmatter

% This macro creates the standard UoB declaration; on the printed hard-copy,
% this must be physically signed by the author in the space indicated.

\makedecl

% LaTeX automatically generates a table of contents, plus associated lists 
% of figures, tables and algorithms.  The former is a compulsory part of the
% dissertation, but if you do not require the latter they can be suppressed
% by simply commenting out the associated macro.

\tableofcontents
\listoffigures
\listoftables
\listofalgorithms
\lstlistoflistings

% The following sections are part of the front matter, but are not generated
% automatically by LaTeX; the use of \chapter* means they are not numbered.

% -----------------------------------------------------------------------------

\chapter*{Executive Summary}

{\bf A compulsory section, of at most $1$ page} 
\vspace{1cm} 

\noindent
The research hypothesis of this study is that probabilistic principal component analysis with the
Gaussian Process Latent Variable Model is applicable for improving the creative production workflow of complex 3D geometry such as hair structures of humanoids.

The topic of this thesis explores the concept of assisted content generation by machine learning for the production of 3D hair geometry. The production of 3D virtual worlds is a time-consuming and costly process that also demand expert knowledge. 3D assets encompass a vast range of applications, ranging from simulations and research, to contributing towards the functioning of many businesses. The production of 3D assets also plays a pivotal role in engineering design, and the provisioning of entertainment. One particular task is the creation of 3D hair geometry for humanoid characters. The production of 3D hair is arduous as hair structure is a complex system containing much interdependence between components.

Machine learning applications typically use large data sets for training on problems that often have a concise answer for a given prediction. The application of machine learning to enhance production for creative work is an exciting field that tackles new challenges - not only could it have a notable impact on the economy, artistic products tend to have small sets of data available and evaluation of quality is subjective. Given the same input, acceptable solutions can vary significantly. The mentioned peculiarities of applying machine learning for 3D mesh data defines a unique field of problems to investigate.

Existing tools for 3D modelling have remained mostly static in the paradigm of approach over the past few decades. Automation through methods such as procedural generation can produce output much faster, but the lack of control over the final result makes it less desirable than traditional methods of 3D modelling. The focus of this project is to formulate a revolutionary framework that improves the workflow of producing 3D hair geometry.

\begin{quote}
\noindent
\begin{itemize}
\item Resolved the alignment problem by coming up with a generative model for approximating hair structure
\item Demonstrated the use of GPLVM on various sizes of data set
\item Implemented an add-on package for a 3D production program, Blender
	\begin{itemize}
		\item This implementation creates guiding splines that are useful for generating geometry
		\item Appropriate for small training set that is practical for content creators
		\item Real-time performance that matches current processing time required by traditional tools
	\end{itemize}
\item [TODO]: Evaluating kernels
\item [TODO]: Analysis and Evaluation
\end{itemize}
\end{quote}

% -----------------------------------------------------------------------------

\chapter*{Supporting Technologies}

{\bf A compulsory section, of at most $1$ page}
\vspace{1cm} 

\noindent
This section should present a detailed summary, in bullet point form, 
of any third-party resources (e.g., hardware and software components) 
used during the project.  Use of such resources is always perfectly 
acceptable: the goal of this section is simply to be clear about how
and where they are used, so that a clear assessment of your work can
result.  The content can focus on the project topic itself (rather,
for example, than including ``I used \mbox{\LaTeX} to prepare my 
dissertation''); an example is as follows:

\begin{quote}
\noindent
\begin{itemize}
\item I used the Java {\tt BigInteger} class to support my implementation 
      of RSA.
\item I used a parts of the OpenCV computer vision library to capture 
      images from a camera, and for various standard operations (e.g., 
      threshold, edge detection).
\item I used an FPGA device supplied by the Department, and altered it 
      to support an open-source UART core obtained from 
      \url{http://opencores.org/}.
\item The web-interface component of my system was implemented by 
      extending the open-source WordPress software available from
      \url{http://wordpress.org/}.
\end{itemize}
\end{quote}

% -----------------------------------------------------------------------------

\chapter*{Notation and Acronyms}

{\bf An optional section, of roughly $1$ or $2$ pages}
\vspace{1cm} 

\noindent
Any well written document will introduce notation and acronyms before
their use, {\em even if} they are standard in some way: this ensures 
any reader can understand the resulting self-contained content.  

Said introduction can exist within the dissertation itself, wherever 
that is appropriate.  For an acronym, this is typically achieved at 
the first point of use via ``Advanced Encryption Standard (AES)'' or 
similar, noting the capitalisation of relevant letters.  However, it 
can be useful to include an additional, dedicated list at the start 
of the dissertation; the advantage of doing so is that you cannot 
mistakenly use an acronym before defining it.  A limited example is 
as follows:

\begin{quote}
\noindent
\begin{tabular}{lcl}
AES                 &:     & Advanced Encryption Standard                                         \\
DES                 &:     & Data Encryption Standard                                             \\
                    &\vdots&                                                                      \\
${\mathcal H}( x )$ &:     & the Hamming weight of $x$                                            \\
${\mathbb  F}_q$    &:     & a finite field with $q$ elements                                     \\
$x_i$               &:     & the $i$-th bit of some binary sequence $x$, st. $x_i \in \{ 0, 1 \}$ \\
\end{tabular}
\end{quote}

% -----------------------------------------------------------------------------

\chapter*{Acknowledgements}

{\bf An optional section, of at most $1$ page}
\vspace{1cm} 

\noindent
It is common practice (although totally optional) to acknowledge any
third-party advice, contribution or influence you have found useful
during your work.  Examples include support from friends or family, 
the input of your Supervisor and/or Advisor, external organisations 
or persons who  have supplied resources of some kind (e.g., funding, 
advice or time), and so on.

% =============================================================================

% After the front matter comes a number of chapters; under each chapter,
% sections, subsections and even subsubsections are permissible.  The
% pages in this part are numbered with Arabic numerals.  Note that:
%
% - A reference point can be marked using \label{XXX}, and then later
%   referred to via \ref{XXX}; for example Chapter\ref{chap:context}.
% - The chapters are presented here in one file; this can become hard
%   to manage.  An alternative is to save the content in seprate files
%   the use \input{XXX} to import it, which acts like the #include
%   directive in C.

\mainmatter

% -----------------------------------------------------------------------------

\chapter{Contextual Background}
\label{chap:context}

\section{Topic Background}
\noindent
3D Representations in Computer Graphics
In computer graphics, 3D objects are represented in many forms. 3D scanners capture raw data in various forms such as point clouds, range images, and voxels. A point cloud is a collection of 3D points, often used in computer vision.  A range image maps pixels of a depth image to a set of points in the scene. Voxels are units of cubes that define the volume of objects, it has applications in many fields including medicine where voxels are used to visualise the results of MRI scans of patients.\cite{mri}

In a production environment it is simpler to define geometry as opposed to capturing examples. The most common representation used for CG production are polygonal meshes, data that contains information of vertices, edges, and faces. Topology is the organisation of the components that define the mesh geometry. Two surfaces with the same appearance could have different topologies. Where precision is concerned, parametric definitions are used for industries such as CAD. Every representation has its advantages depending on the use case. It is possible to convert between representations, but data loss may be incurred. Properties that make polygon meshes desirable include being efficient rendering, simple to define, expressive enough to capture geometry required, and works well with established techniques such as UV texture mapping and deforming algorithms. The rendering pipeline often converts meshes to tri-faces (faces constructed by three edges) as an optimisation process, but best practice for 3D artists is to maintain a topology of quad-faces which are easier to organise and conforms better with editing tools and algorithms.

%** Image of representations

\section{Production of 3D Hair Geometry}
\noindent
On average, a human is born with about 100,000 scalp hair follicles. It is very expensive to render and animate physically correct hair, but creative liberties have been taken to approximate or stylize 3D hair such that it is both acceptable aesthetically and feasible in terms of performance. This study considers modelling of hair geometry, the motion of hair is assumed to be its default resting pose.

In recent years, impressive 3D hair solutions for real-time simulation of realistic hair and fur, such as Nvidia HairWorks and AMD’s TressFX has emerged. These solutions, however, have limited application in comparison to their traditional counterpart of polygonal hair. It is often the case that texture-mapped polygonal hair is used as a fallback for when the advanced simulation fails. Realism is not necessarily always desirable, polygon hair can flexibly represent different art styles. In some cases, a blend of multiple representations are used to balance between cost and quality. 3D hair in movies with large budget can afford to render hair with much higher fidelity for important characters, but might use efficient variants for scenarios such as crowd simulation. Ultimately, representation of virtual hair generally follows a structure of splines with control points that define the overall organisation of strands or segments.


[Image of Hair Geometry]

\section{Procedural Generation and Automated Production}
\noindent
Procedural generation techniques has been successfully used for terrains and city modelling. Methods such as the Lindenmayer system are used for foliage generation. Fractals with simulated noise can create structure that resemble patterns observed in nature. Procedural techniques, however, are seldom used for modelling objects with specific features. It is difficult to control the output of procedurally generated content without heavily restricting its capabilities.
\cite{procedural1}
\cite{procedural2}

Machine Learning in 3D Production
Machine learning methods can use existing production as training input to train models that are more concise and versatile than predetermined procedural approaches. Research in the past has used machine learning to help non-artists drawing 2D images. Gaussian processes enabled stylised inverse kinematics that deliver simple controls for complicated animation by learning the posing of joints. 
\cite{MLMethods}

\section{Motivation and Significance}
\noindent
State of the art 3D production software such as AutoDesk Maya, 3DS Max, and Blender are advanced programs with sophisticated list of features. That said, such programs have extremely convoluted user interfaces, even the most experienced professionals do not recognise each and every tool available. The most versatile tools are generally the most basic that perform atomic changes as they are applicable in every scenario. Examples include selection of primitives such as vertices, edges, or faces and performing translation, rotation, and scaling. Sculpting tools moves many data points simultaneously, they are popular for defining organic surfaces now that modern machines are sufficiently powerful. Experienced artists might search for an existing base mesh that is similar to start from, but it is not always the case that such a base mesh exists - there are also concerns for quality, such as unorganised topology. As the geometry becomes more detailed and well-defined, each alteration makes less impact and the space of sensible edits becomes smaller. The design and production of 3D geometry remains a slow and delicate process.

Virtual hair creation is a necessity for characters of CG movies and video games that are embedded within culture both economically and as entertainment. Specialised artists learn to be proficient with the design of hair, variety of styles, and techniques for creating them. Hair geometry is much more concentrated than other types, containing many data points that are tedious to edit. Soft selection and sculpting tools are good enough for defining the structure but maintaining topology and issues such as overlapping surfaces are still problematic. Learning the relation of hair structure allows the potential of discovering new hairstyles.  It can also be used as a mean of rapidly generating initial base geometry that fits the target output better than existing geometry found. Generative methods could ensure a level of quality, clean topology that fits established specifications. Assisted content generation using machine learning provides a convenient, non-intrusive and intuitive method for rapidly generating new hair geometry from existing data.

%**Move to evaluation
The application of machine-learning based tools could enhance the workflow of professional users and improve the experience for non-expert consumers. Such tools integrate into the production environment to improve the efficiency of acquiring initial base geometry and visually compare designs during pre-production. Non-expert users receive the ability to produce 3D geometry without requiring to learn the intrinsics of traditional 3D modelling software. The rise in popularity for augmented reality and 3D printing inspires the development of generative tools that are intuitive and simplistic to use. Applications that allow users to create their personal content could also integrate machine-learning based tools to prevent inappropriate or undesirable creation from being produced while providing options that surpass existing alternatives. An example would be avatar creation for many applications and video games. A space of reasonable options generated from predefined outputs by the developers will allow users to interpolate between sensible configurations, providing an excellent level of customisation while adhering to defined constraints.

\section{Challenges}
\noindent
This study faces a number of challenges. First of all, 3D meshes are difficult to compare. The training data in its raw form will have varying dimensions. Meshes can be viewed as samples of the true geometry, thus meshes that represent the same object could differ drastically in number of data points depending on its level of detail. Typical feature extraction methods do not work well on meshes as artistic products are sensitive to data loss - any change could affect the final result drastically.

Another problem encountered is the lack of training data. Typical machine learning solutions use huge data sets in the order of hundreds of thousands for training, but for 3D meshes the expected size of readily available training data is much smaller. Public repositories of 3D polygonal hair are generally around a few thousand in size. Studios that store and organise past production could likely match the size of public repositories, depending on the size of the company. Independent artists that keep their production will range in the hundreds.
\cite{tsr}

The application of machine learning methods must also account for subjectivity of evaluating artistic assets. The range of acceptable solutions is ambiguous, likened to how hair styles of characters can change drastically during the design phase, determining the threshold of acceptable solutions will be in itself a challenge to resolve.

As mentioned previously, 3D meshes are delicate and can easily be invalidated from small changes. Thus, reparations are required to ensure that the output of trained models are acceptable.

In a production environment, the time required for a technique to return observable result directly affects throughput. For practical efficacy of assisted content generation, usage of our methods should be reasonably effective.

\section{Central Objectives}
\noindent
The aim of this study is …
\begin{itemize}
\item Resolving the alignment problem of 3D data by standardisation.
\item Explore the application of GPLVM for 3D hair geometry in a production pipeline.
\item Investigate the use of latent variables for identifying stylistic properties of 3D hair geometry.
\item Demonstrate the use of non-linear manifold to generate new hairstyles from training data.
\item Enable an intuitive method for non-experts to create 3D hair geometry.
\item Observable output demand performance close to real-time for practical use.
\end{itemize}


% -----------------------------------------------------------------------------

\chapter{Technical Background}
\label{chap:technical}

\section{Formal Definition of 3D Polygon Mesh Representation}
%\cite{polyhedron}The winged-edge polyhedron representation proposed by Baumgart in 1972 introduced a mesh data structure to represent 3D geometry using vertices, edges, and faces. \cite{wingededge}

Polygon mesh representation of 3D surfaces are composed of vertices, edges, and faces. Let polygon mesh $P = (\mathbf{V, E, F})$, where $\mathbf{V, E, F}$ represents the set of vertices, edges, and faces respectively. In practice, polygonal meshes contain more components that affect surface appearance such as texture coordinates and vertex normals, however, the components described are sufficient for geometric processing.

\subsection{Mesh Vertices}
A mesh vertex $v$ is a 3D point of the form $\forall (x, y, z) \in \Re, v = (x, y, z)$.
The set of vertices is a point cloud representation of the geometry. 

\subsection{Edges}
An edge $e$ is an unordered pair that connects two vertices. Formally, it is described in the form $\forall (v_1, v_2) \in \mathbf{V}, e = \{v_1, v_2\}$. Vertices connected by edges form a wireframe of the geometry. 

\subsection{Polygon Faces}
A polygon face can be formed from an arbitrary number of vertices $\forall (v_1, v_2,...,v_n) \in \mathbf{V}, f_n = (v_1, v_2,..., v_n)$, however, in this context we are only concerned with tri-faces $\forall (v_1, v_2, v_3) \in \mathbf{V}, f_3 = (v_1, v_2, v_3)$ and quad-faces $\forall (v_1, v_2, v_3, v_4) \in \mathbf{V}, f_4 = (v_1, v_2, v_3, v_4)$. Faces describe the geometric surface of an object. 

\subsection{Edge Loops}
3D programs often allow edge loop selection which are useful properties of the geometry. An edge loop is defined (on blender) as a set of connected edges that either forms a loop or the end vertices are poles (vertices that do not have edges). Edge loops are useful for extracting more information on the structure of the mesh.
\cite{edgeloops}

\section{Principal Component Analysis}
In multivariate analysis, principal component analysis (PCA) is a statistical technique used to perform dimensionality reduction within a data set that consists of interrelated variables.
The objective is to identify the principal components, a new set of uncorrelated variables ordered by variation present in the original variables, thus, minimising the loss of information.\cite{pca2002}
%*suppose vector X
The first principal component is selected by searching for a linear function with maximum variance within the observed data set.
%**formula
Following principal components are found by searching for linear functions with maximum variance and are uncorrelated with previously identified principal components.
%**formula
This transformation captures most of the variation linearly from the original data set with a chosen dimension that is generally less than the original.

\subsection{PCA *Main}
In multivariate analysis, principal component analysis (PCA) is a statistical technique used to perform dimensionality reduction. It was originally introduced by Pearson in 1901\cite{pca1901}, and independently developed by Hotelling\cite{pca1933} in 1933, presenting one of the most common derivation of PCA in algebraic terms as a standardized linear projection which maximizes the variance in projection space.

Intro: Consider a set of $n$ observed variables $\mathbf{X}=\{x_1,x_2,...,x_n\}$. The variables may be correlated. For example hair structure: position, orientation, length. The motivation of PCA is to find a smaller set of independent variables $\mathbf{Y}=\{y_1,y_2,...,y_m\}$ where generally $m<n$, that is descriptive of the values $\mathbf{X}$ would take.

The relation would thus be $x_i=f_i(y_1,y_2,...), (i=1,2,...,n)$.
the ys are components.
Consider only normally distributed systems of components having zero correlations and unit variacnes.
Denote the expectation with $E$, the mean value in the population, the condition of zero mean is expressed by $E_{yi}=0$.
Combining the assumption of unit variance and zero correlation: $E_{yiyj}=\delta_{ij}$, where $\delta_{ij}$ is the Kronecker delta, euqals unity if $i$ equals $j$, zero otherwise.

FOllowing notation of T.L. Kelley, x's standard measures are expressed by taking the deviation of each from its mean value and dividing by its standard deviation. We get a set of quantities $\mathbf{Z}=\{z_1,z_2,...,z_n\}$. Confining to the case in which the functions $f_i$ are linear, the equations take form of 
\begin{equation} \label{pca:line}
z_i=\sum_ja_{ij}y_j
\end{equation}
, constant terms disappearing because both the z's and y's have zero means.

Squaring each side of equation \ref{pca:line} and taking the mean value, it is evident that the variance of $z_i$ may be written $a^2_{i1}+a^2_{i2}+...+a^2_{in}$, and that the first term is correctly described as the contribution of $y_1$ to the variance $z_i$. The sum of the contributions of $y_i$ to the variances of all the $z$'s is $S = a^2_{11}+a^2_{21}+...+a^2_{n1}$, which in our abbreviated notation may be written $S=a_{i1}a_{i1}$.


Derivation: 

\subsection{PCA}
The most common derivation of PCA is in terms of a standardized linear projection which maximises the variance in the projected space (Hotelling, 19333). For a set of observed d-dimensional data vectors $\{\mathbf{t}_n\}$, $n\in {1,...,N}$, the q principal axes $\mathbf{w}_j, j\in \{1,...,q\}$, are those orthonormal axes onto which the retained variance under projection is maximal. It can be shown that the vectors $\mathbf{w}_j$ are given by the q dominant eigenvectors (those with the largest associated eigenvalues $\lambda_j$) of the sample covariance matrix $\mathbf{Sw}_j=\lambda_j\mathbf{w}_j$. The q principal components of the observed vector $\mathbf{t}_n$ are given by the vector $\mathbf{x}_n=\mathbf{W}^T(\mathbf{t}_n-\mathbf{\bar{t}})$, where $\mathbf{W=(w_1,w_2,...,w}_q)$. The variables $x_j$ are then uncorrelated such that the covariance matrix $\sum_n\mathbf{x}_n\mathbf{x}^T_n /N$ is diagonal with elements $\lambda_j$.

A complementary property of PCA is that of all orthogonal linear projections $\mathbf{x}_n=\mathbf{W}^T(\mathbf{t}_n-\mathbf{\bar{t}})$, the principal component projection minimises the squared reconstruction error $\sum_n||\mathbf{t}_n-\mathbf{\hat{t}}_n||^2$, where the optimal linear reconstruction of $\mathbf{t}_n$ is given by $\mathbf{\hat{t}=Wx}_n+\mathbf{\bar{t}}$.

\section{Probabilistic Principal Component Analysis}
The probabilistic principal component analysis (PPCA) introduced by Tipping and Bishop address the limitation of PCA by deriving it within a density estimation framework.\cite{ppca}

%A probabilistic formulation of PCA is obtained from a Gaussian latent variable model.
A latent variable model relates a d-dimensional observation vector t to a corresponding q-dimensional vector of latent (unobserved) variables x.

\subsection{Latent Variable Models}
(B. Everett 1984 Into to Latent Variable Models).	
A latent variable model seeks to relate d-dimensional observation vector t to a corresponding q-dimensional vector of latent variables x.

\subsection{Factor Analysis}
Factor analysis is a latent variable model where the relationship is linear.
\begin{equation}
	\mathbf{t=Wx+\mu+\epsilon}
\end{equation}
The d x q matrix W relates two sets of variables, while the parameter vector $\mu$ permits the model to have non-zero mean. The motivation is that q < d, the latent variables will represent observed data with less dimensions by choosing variables that offer more concise explanation. 

Conventionally, the Gaussian distribution is $\mathbf{x}\sim\mathcal{N}(\mathbf{0,I})$, and the latent variables are defined to be independent and Gaussian with unit variance. By additionally specifying the error, or noise, model to also be Gaussian $\epsilon \sim\mathcal{N}(\mathbf{0, \Psi})$, the relationship equation induces a corresponding Gaussian distribution for the observations
$\mathbf{t\sim\mathcal{N}(\mu, \mathbf{WW}^T+\Psi)}$. The models parameters may thus be determined by maximum likelihood. As there is no closed form analytic solution for $\mathbf{W}$ and $\Psi$, their values are obtained via an iterative procedure.

The motivation for factor analysis model is that by constraining the error covariance $/Psi$ to be a diagonal matrix whose elements $\psi_i$ are usually estimated from the data,the observed variables $t_i$ are \textit{conditionally independent} given the values of the latent variables $\mathbf{x}$. These latent variables are thus intended to explain the correlations between observation variables while $/epsilon_i$ represents variability unique to a particular $t_i$. This is where factor analysis differs from PCA, which effectively treats covariance and variance identically.

\subsection{PPCA}
\subsubsection{The probability model}
The use of isotropic Gaussian noise model $\mathcal{N}(\mathbf{0,\sigma^2I})$ for $\epsilon$ in conjunction with equation (1) implies that the $\mathbf{x}$ conditional probability distribution over t-space is given by 
\begin{equation}
	\mathbf{t|x\sim\mathcal{N}(Wx+\mu,\sigma^2I)}
\end{equation}
With the marginal distribution over the latent variables also Gaussian and conventionally defined $\mathbf{x\sim\mathcal{N}(0,I)}$, the marginal distribution for the observed data t is readily obtained by integrating out the latent variables and is likewise Gaussian:
\begin{equation}
	\mathbf{t\sim\mathcal{N}(\mu,C)}
\end{equation}
where the observed covariance model is specified by $\mathbf{C=WW}^T+\sigma^2\mathbf{I}$.
The corresponding likelihood is then
\begin{equation}
	\mathcal{L}=\frac{N}{2}\{d ln(2\pi)+ln|\mathbf{C}+tr(\mathbf{C^{-1}S})\}
\end{equation}
where
\begin{equation}
	\mathbf{S}=\frac{1}{N}\sum^N_{n=1}(\mathbf{t}_n-\mu)(\mathbf{t}_n-\mu)^T
\end{equation}
The maximum likelihood estimator for $\mu$ is given by the mean of the data, in which case $\mathbf{S}$ is the sample covariance matrix of the observations $\{\mathbf{t}_n\}$. Estimates for $\mathbf{W}$ and $\sigma^2$ may be obtained by iterative maximisation of $\mathcal{L}$. In contrast with factor analysis, maximum likelihood estimators for $\mathbf{W}$ and $\sigma^2$ may be obtained explicitly.

The conditional distribution of the latent variables $\mathbf{x}$ given the observed $\mathbf{t}$, which may be calculated by using Bayes theorem and is again Gaussian.
\begin{equation}
	\mathbf{x|t\sim\mathcal{N}(M^{-1}W}^T\mathbf{(t-\mu),\sigma^2M^{-1})}
\end{equation}
where we have defined $\mathbf{M=W}^T\mathbf{W+\sigma^2I}$.
Note that $\mathbf{M}$ is of size q x q and whereas C is d x d.

\subsubsection{Properties of the maximum likelihood estimators}
With C given by  $\mathbf{WW}^T+\sigma^2\mathbf{I}$, the likelihood is maximised when
\begin{equation}
	\mathbf{W}_{ML}=\mathbf{U}_q(\Lambda	_q-\sigma^2\mathbf{I})^\frac{1}{2}\mathbf{R}
\end{equation}
where the q column vectors in the d x d matrix $U_q$ are the principal eigenvectors of S, with corresponding eigenvalues $\lambda_1,...,\lambda_q$ in the q x q diagonal $\Lambda_q$ and $R$ is an arbitrary q x q orthogonal rotation matrix. Other combinations of eigenvectors (non-principal) correspond to saddlepoints of the likelihood function. Thus, the latent variable model defined effects a mapping from the latent space into the principal subspace of the observed data.

It may also be shown that for $W=W_{ML}$ the maximum likelihood estimator for $\sigma^2$ is given by
\begin{equation}
	\sigma^2_{ML}=\frac{1}{d-q}\sum^d_{j=q+1}\lambda_j
\end{equation}
which has clear intepretation as the variance 'lost' in the projection, averaged over the lost dimensions.

To find the most likely model given S first estimate $\sigma^2_{ML}$ from one of the equations 8?. When $W_{ML}$ from equation 7?, where for simplicity we would effectively ignore R (choose R = I). Alternatively, employ EM where R at convergence can be considered arbitrary.

\subsubsection{Dimensionality reduction}
Dimensionality reduction form the perspective of PPCA gives the distribution of the latent variables, conditioned from the observation. The distribution may be summarised by its mean.
\begin{equation}
	<x_n|t_n>=M^{-1}W^T_{ML}(t_n-\mu)
\end{equation}
From expression 6?, the corresponding conditional covariance is given by $\sigma^2_{ML}\mathbf{M}^{-1}$ and is thus independent of n. It can be seen that when $\sigma^2\rightarrow 0,\mathbf{M}^{-1}\rightarrow (\mathbf{W}^T_{ML}\mathbf{W}_{ML})^{-1}$ and equation 9? then represents an orthogonal projection into latent space and so standard PCA is recovered. However, the density model then becomes singular, and thus undefined. In practice, with $\sigma^2 > 0$ as determined by equation 8?, the latent projection becomes skewed towards the origin as a result of the Gaussian marginal distribution for $\mathbf{x}$. Because of this, the reconstruction $\mathbf{W}_{ML}\mathbf{x}_n|\mathbf{t}_n>+\mu$ is not an orthogonal projection of $\mathbf{t}_n$ and is therefore not optimal (in the squared reconstruction error sense). Nevertheless, optimal reconstruction of the observed data from the conditional latent mean may still be obtained, in the case of $\sigma^2 > 0$, and is given by $\mathbf{W}_{ML}(\mathbf{W}^T_{ML}\mathbf{W}_{ML}^{-1}\mathbf{M}<\mathbf{x}_n|\mathbf{t}_n>+\mu$.

\subsubsection{PPCA and missing data}
PPCA offers a natural approach to the estimation of the principal axes in cases where some, or indeed all, of the data vectors $mathbf{t}_n=(t_{})$ exhibit one or more missing (at random) values. Drawing on the standard methodology for maximising the likelihood of a Gaussian model in the presence of missing values (Little and Rubin, 1987) and EM algorithm for PPCA given in Appendix B, we may derive an iterative algorithm for maximum likelihood estimation of the principal axes, where both the latent variables $\{\mathbf{x}_n\}$ and the missing observation $\{t_{nj}\}$ make up the 'complete' data. Even with the presence of uncertainty from missing data, salient features of the projection remain clear.

\vspace{10cm}
%* BEGIN GPLVM PPCA RECAP
PPCA is a latent variable model in which the maximum likelihood solution for the parameters is found through solving a eigenvalue problem on the data's covariance matrix.
Given set of centred D-dimensional data $\mathbf{Y}=[\mathbf{y}_1...\mathbf{y}_N]^T$. Denote q-dimensional latent variable associated with each point in $\mathbf{x}_n$
The relationship between the latent variable and the data point is linear with noise added,
\begin{equation}
\mathbf{y}_n=\mathbf{Wx}_n+\eta_n
\end{equation}
where the matrix $\mathbf{W}\in\Re^{D\times q}$ specifies the linear relationship between the between the latent-space and the data space and the noise values, $\eta_n\in\Re^{D\times 1}$, are taken to be an independent sample from a spherical Gaussian distribution with mean zero and covariance $\beta^{-1}\mathbf{I}$,
\begin{equation}
p(\eta_n)=N(\eta_n|\mathbf{0},\beta^{-1}\mathbf{I})
\end{equation}
The likelihood for a data point can then be written as 
\begin{equation}
p(\mathbf{y}_n|\mathbf{x}_n,\mathbf{W},\beta)=N(\mathbf{y}_n|\mathbf{Wx}_n,\beta^{-1}\mathbf{I})
\end{equation}
To obtain the marginal likelihood, integrate over the latent variables
\begin{equation}
p(\mathbf{y}_n|\mathbf{W},\beta)=\int p(\mathbf{y}_n|\mathbf{x}_n, \mathbf{W},\beta)p(\mathbf{x}_n),d\mathbf{x}_n
\end{equation}
which requires to specify a prior distribution over $\mathbf{x}_n$. For probabilistic PCA, the appropriate prior is a unit covariance, zero mean Gaussian distribution,
\begin{equation}
p(\mathbf{x}_n)=N(\mathbf{x}_n|\mathbf{0},\mathbf{I})
\end{equation}
The marginal likelihood for each data point can then be found analytically (through marginalisation) as
\begin{equation}
p(\mathbf{y}_n|\mathbf{W},\beta)=N(\mathbf{y}_n|0,\mathbf{WW}^T+\beta^{-1}\mathbf{I})
\end{equation}
Taking advantage of the independence of the data points, the likelihood of the full data set is given by 
\begin{equation}
p(\mathbf{Y}|\mathbf{W},\beta)=\prod^N_{n=1}p(\mathbf{y}_n|\mathbf{W},\beta)
\end{equation}
Parameters $\mathbf{W}$ can the be found through maximisation of the full data set.
%* END GPLVM PPCA RECAP



\section{Gaussian Process Latent Variable Model}

GPs. Cite Rasmussen and Williams 2006. Stochastic processes, Nonparametric, nonlinear, supervised, etc.
GP-LVM. Cite Lawrence 2005. Unsupervised\cite{gplvm}
GP-LVM can be considered as a multiple-output GP regression model where only the output data are given.
THe inputs are unobserved latent variables.
Instead of integrating, they are optimised. This makes model tractable. It is a nonlinear extension of the linear PPCA. 
Discuss kernels.

The Gaussian Process Latent Variable Model (GP-LVM) probabilistic framework to perform non-linear principal component analysis. 

Three categories of ML: supervised, unsupervised, and reinforcement learning.
One approach to unsupervised learning is to represent the data, $\mathbf{Y}$, in some lower dimensional embedded space, $\mathbf{X}$. GP-LVM is a method that represents the data in this latent space.

A latent variable model relates a set of latent variables, $\mathbf{X}\in \Re^{N\times q}$, to a set of observed variables, $\mathbf{Y}\in \Re^{N\times D}$, through a set of parameters. A typical latent variable model marginalises the latent variables and finds the parameters through maximising the likelihood. GP-LVM alternatively marginalises the parameters and optimise the latent variables. The results are equivalent.

\subsection{Dual Probabilistic PCA}
Lawrence introduced the dual probabilistic PCA (DPPCA) as an alternative solution to PPCA that is equivalent. Instead of optimising parameters and marginalising latent variables, the dual approach marginalises the parameters, $\mathbf{W}$, and optimises with respect to latent variables, $\mathbf{X}$.

A Bayesian methodology requires a prior of $\mathbf{W}$. The spherical Gaussian distribution is a simple choice for illustration.
\begin{equation}
	p(\mathbf{W})=\prod^D_{i=1}\mathcal{N}(\mathbf{w}_i|\mathbf{0,I})
\end{equation}
The marginalised likelihood of $\mathbf{W}$ is
\begin{equation}
	p(\mathbf{Y|X},\beta)=\prod^D_{i=1}p(\mathbf{y}_{:,d}|\mathbf{X},\beta)
\end{equation}
where $\mathbf{y}_{:,d}$ represents the dth column of Y and
\begin{equation}
	p(\mathbf{y}_{:,d}|\mathbf{X},\beta)=\mathcal{N}(\mathbf{y}_{:,d}|\mathbf{0,XX}^T+\beta^{-1}\mathbf{I})
\end{equation}

The objective function is the log-likelihood
\begin{equation}
	L=-\frac{DN}{2}ln2\pi-\frac{D}{2}ln|\mathbf{K}|-\frac{1}{2}tr(\mathbf{K^{-1}YY}^T)
\end{equation}
The gradients with respect to $\mathbf{X}$ may be found (Magnus and Neudecker, 1999) as
\begin{equation}
	\frac{\sigma L}{\sigma \mathbf{X}}=\mathbf{K^{-1}YY}^T\mathbf{K^{-1}X}-D\mathbf{K^{-1}X},
\end{equation}
a fixed point where the gradients are zero is then given by 
\begin{equation}
	\frac{1}{D}\mathbf{YY}^T\mathbf{K^{-1}X=X}.
\end{equation}
The values for $\mathbf{X}$ which maximise the likelihood are given by 
\begin{equation}
	\mathbf{X=ULV}^T
\end{equation}
where $\mathbf{U}$ is an $N\times q$ matrix whose columns are the first eigenvectors of $\mathbf{YY}^T,\mathbf{L}$ is a $q\times q$ diagonal matrix whose $jth$ element is $l_j=(\lambda_j-\frac{1}{\beta})^{-\frac{1}{2}}$ where $\lambda_j$ is the eigenvalue associated with the $jth$ eigenvector $D^{-1}\mathbf{YY}^T$ and $V$ is an arbitrary $q\times q$ rotation matrix. Assuming that the eigenvalues are ordered according to magnitude with largest first. The eigenvalue problem developed is equivalent to that solved in PCA. %See appendix of Lawrence 2005.
The formulation of PCA in this manner is a key step in the development of kernel PCA (Scholkopf et al 1998) where the matrix of inner products $\mathbf{YY}^T$ is replaced with a kernel (Tipping 2001 for derivation). The dual PPCA model is equivalent to PPCA, but optimises the latent variables and marginalises the parameters instead of marginalising the latent variables and optimising the parameters.

\subsection{Gaussian Processes}
Gaussian processes (O'Hagan, 1992; Williams, 1998) are a class of probabilistic models which specify distribution over funciton spaces.

GPs work over infinite dimensions, they are not functions as processes approximate over a finite range.

A GP first requires specifying a Gaussian process prior, parametrised by a mean and covariance.

A simple GP prior over the space of functions that are linear but corrupted by Gaussian noise of variance $\beta^{-1}\mathbf{I}$ is
\begin{equation}
	k(\mathbf{x}_i,\mathbf{x}_j)=\mathbf{x}^T_i\mathbf{x}_j+\beta^{-1}\mathbf{I}
\end{equation}
where $\mathbf{x}_i$ and $\mathbf{x}_j$ are vectors from the space of inputs to the function and $\sigma_1{ij}$ is the Knronecker delta. If these inputs were taken from our embedding matrix, $\mathbf{X}$, and the covariance function was evaluated at each of the $N$ points, we would recover the covariance matrix of the form
\begin{equation}
	\mathbf{K=XX}^T+\beta^{-1}\mathbf{I}
\end{equation}
where the element at $ith$ row and $jth$ column ok $\mathbf{K}$ is given by the simple GP prior. This is recognised as the covariance associated with each factor of the marginal likelihood for dual probabilistic PCA. The marginal likelihood for dual probabilistic PCA is therefore a product of D independent Gaussian processes. In PCA we are optimising parameters and input positions of a Gaussian process prior distribution where the (linear) covariance function for each dimension is given by $\mathbf{K}$

\subsection{GP-LVM}
The dual PPCA points to a new class of models which consist of Gaussian process mappings from a latent space, $\mathbf{X}$, to an observed data-space, $\mathbf{Y}$. DPPCA is the special case where the output dimensions are \textit{a priori} assumed to be linear, independent and identically distributed. However, each of these assumptions can be infringed to obtain new probabilistic models. Independence can be broken by allowing an arbitrary rotation on the data matrix $\mathbf{Y}$, the 'identically distributed' assumption can be broken by allowing different covariance functions for each output dimension. By replacing the inner product kernel with a covariance function that allows for non-linear functions we obtain a non-linear latent variable model. Due to the close relationship with the linear model, which has an intepretation as probabilistic PCA, such a model can be interpreted as a non-linear probabilistic version of PCA.

\section{Bayesian Gaussian Process Latent Variable Model}
\iffalse
	%* START BGPLVM RECAP OF TRADITIONAL GPLVM
	Let $Y \in \Re^{N \times D}$ be the observed data where $N$ is the number of observations and $D$ is the dimensionality of each data vector. These data are associated with latent variables $X \in \Re^{N \times Q}$ where, for the purpose of dimensionality reduction, $Q \ll D$. GP-LVM defines a forward mapping from the latent space to observation space that is governed by Gaussian processes. If the GPs are taken to be independent across the features then the likelihood function is written as 
	\begin{equation} \label{gplvmml}
		p(Y|X)=\prod^{D}_{d=1}p(y_d|X)
	\end{equation}
	where $y_d$ represents the $d^{th}$ column of $Y$ and
	\begin{equation}
		p(y_d|X)=\mathcal{N}(y_d|\mathbf{0}, K_{NN}+\beta^{-1}I_{N})
	\end{equation}
	$K_{NN}$ is the $N\times N$ covariance matrix defined by the kernel function $k(x,x')$.
	
	Equation \ref{gplvmml} can be viewed as the likelihood function of a multiple-output GP regression model where the vectors of different outputs are drawn independently from the same Gaussian process prior which is evaluated at the inputs $X$. Since X is a latent variable, we can assign it to a prior density given by the standard normal density. More precisely, the prior for $X$ is:
	\begin{equation}
		p(X)=\prod^N_{n=1}\mathcal{N}(\mathbf{x}_n|\mathbf{0}, I_Q)
	\end{equation}
	where each $X_n$ is the $n^{th}$ row of $X$. The joint probability model for the GP-LVM is
	\begin{equation}
	p(Y,X)=p(Y|X)p(X)
	\end{equation}
	The hyperparameters of the model are kernel parameters $\mathbf{\theta}=(\sigma^2_f, \sigma_1,...,\sigma_Q)$ and the inverse variacne parameter $\beta$. These parameters are omitted from the conditioning of the distribution. GPLVM training finds the MAP estimate of $X$ whilst jointly maximising with respect to the hyperparameters.
	%*End GPLVM RECAP
	The Bayesian Gaussian Process Latent Variable Model (Bayesian GP-LVM) extends the GP-LVM by variationally integrating out the input variables of the Gaussian process to compute a lower bound on the exact marginal likelihood of the nonlinear latent variable model, thus becoming robust to overfitting. \cite{bgplvm}
	
	To apply variational Bayes to GP-LVM, need to approximately integrate out latent/input variables that appear nonlinearly in the inverse kernel matrix of the GP model. Standard mean field variational methodologies is not tractable.
	
	A variational Bayesian approach to marginalisation of latent variables, $X$, allows for optimising of the resulting lower bound on the marginal likelihood with respect to the hyperparameters. The lower bound can also be used for model comparison and automatic selection of latent dimensionality.
\fi


\section{Generative Models}

\section{Related Work}
\subsection{Learning a Manifold}
\cite{fontmanifold}


\subsection{Drawing Assistant}

\subsection{AutoHair}
\cite{autohair}
Neural networks
Helicoids
[Automated Production of 3D Assets]
Procedural generation
Generative models
Procedural Generation of Hair 



% -----------------------------------------------------------------------------

\chapter{Project Execution}
\label{chap:execution}

{\bf A topic-specific chapter, of roughly $15$ pages} 
\vspace{1cm} 

\noindent
This chapter is intended to describe what you did: the goal is to explain
the main activity or activities, of any type, which constituted your work 
during the project.  The content is highly topic-specific, but for many 
projects it will make sense to split the chapter into two sections: one 
will discuss the design of something (e.g., some hardware or software, or 
an algorithm, or experiment), including any rationale or decisions made, 
and the other will discuss how this design was realised via some form of 
implementation.  

This is, of course, far from ideal for {\em many} project topics.  Some
situations which clearly require a different approach include:

\begin{itemize}
\item In a project where asymptotic analysis of some algorithm is the goal,
      there is no real ``design and implementation'' in a traditional sense
      even though the activity of analysis is clearly within the remit of
      this chapter.
\item In a project where analysis of some results is as major, or a more
      major goal than the implementation that produced them, it might be
      sensible to merge this chapter with the next one: the main activity 
      is such that discussion of the results cannot be viewed separately.
\end{itemize}

\noindent
Note that it is common to include evidence of ``best practice'' project 
management (e.g., use of version control, choice of programming language 
and so on).  Rather than simply a rote list, make sure any such content 
is useful and/or informative in some way: for example, if there was a 
decision to be made then explain the trade-offs and implications 
involved.

\section{Example Section}

This is an example section; 
the following content is auto-generated dummy text.
\lipsum

\subsection{Example Sub-section}

\begin{figure}[t]
\centering
foo
\caption{This is an example figure.}
\label{fig}
\end{figure}

\begin{table}[t]
\centering
\begin{tabular}{|cc|c|}
\hline
foo      & bar      & baz      \\
\hline
$0     $ & $0     $ & $0     $ \\
$1     $ & $1     $ & $1     $ \\
$\vdots$ & $\vdots$ & $\vdots$ \\
$9     $ & $9     $ & $9     $ \\
\hline
\end{tabular}
\caption{This is an example table.}
\label{tab}
\end{table}

\begin{algorithm}[t]
\For{$i=0$ {\bf upto} $n$}{
  $t_i \leftarrow 0$\;
}
\caption{This is an example algorithm.}
\label{alg}
\end{algorithm}

\begin{lstlisting}[float={t},caption={This is an example listing.},label={lst},language=C]
for( i = 0; i < n; i++ ) {
  t[ i ] = 0;
}
\end{lstlisting}

This is an example sub-section;
the following content is auto-generated dummy text.
Notice the examples in Figure~\ref{fig}, Table~\ref{tab}, Algorithm~\ref{alg}
and Listing~\ref{lst}.
\lipsum

\subsubsection{Example Sub-sub-section}

This is an example sub-sub-section;
the following content is auto-generated dummy text.
\lipsum

\paragraph{Example paragraph.}

This is an example paragraph; note the trailing full-stop in the title,
which is intended to ensure it does not run into the text.

% -----------------------------------------------------------------------------

\chapter{Critical Evaluation}
\label{chap:evaluation}

{\bf A topic-specific chapter, of roughly $15$ pages} 
\vspace{1cm} 

\noindent
This chapter is intended to evaluate what you did.  The content is highly 
topic-specific, but for many projects will have flavours of the following:

\begin{enumerate}
\item functional  testing, including analysis and explanation of failure 
      cases,
\item behavioural testing, often including analysis of any results that 
      draw some form of conclusion wrt. the aims and objectives,
      and
\item evaluation of options and decisions within the project, and/or a
      comparison with alternatives.
\end{enumerate}

\noindent
This chapter often acts to differentiate project quality: even if the work
completed is of a high technical quality, critical yet objective evaluation 
and comparison of the outcomes is crucial.  In essence, the reader wants to
learn something, so the worst examples amount to simple statements of fact 
(e.g., ``graph X shows the result is Y''); the best examples are analytical 
and exploratory (e.g., ``graph X shows the result is Y, which means Z; this 
contradicts [1], which may be because I use a different assumption'').  As 
such, both positive {\em and} negative outcomes are valid {\em if} presented 
in a suitable manner.

% -----------------------------------------------------------------------------

\chapter{Conclusion}
\label{chap:conclusion}

{\bf A compulsory chapter,     of roughly $5$ pages} 
\vspace{1cm} 

\noindent
The concluding chapter of a dissertation is often underutilised because it 
is too often left too close to the deadline: it is important to allocation
enough attention.  Ideally, the chapter will consist of three parts:

\begin{enumerate}
\item (Re)summarise the main contributions and achievements, in essence
      summing up the content.
\item Clearly state the current project status (e.g., ``X is working, Y 
      is not'') and evaluate what has been achieved with respect to the 
      initial aims and objectives (e.g., ``I completed aim X outlined 
      previously, the evidence for this is within Chapter Y'').  There 
      is no problem including aims which were not completed, but it is 
      important to evaluate and/or justify why this is the case.
\item Outline any open problems or future plans.  Rather than treat this
      only as an exercise in what you {\em could} have done given more 
      time, try to focus on any unexplored options or interesting outcomes
      (e.g., ``my experiment for X gave counter-intuitive results, this 
      could be because Y and would form an interesting area for further 
      study'' or ``users found feature Z of my software difficult to use,
      which is obvious in hindsight but not during at design stage; to 
      resolve this, I could clearly apply the technique of Smith [7]'').
\end{enumerate}

% =============================================================================

% Finally, after the main matter, the back matter is specified.  This is
% typically populated with just the bibliography.  LaTeX deals with these
% in one of two ways, namely
%
% - inline, which roughly means the author specifies entries using the 
%   \bibitem macro and typesets them manually, or
% - using BiBTeX, which means entries are contained in a separate file
%   (which is essentially a databased) then inported; this is the 
%   approach used below, with the databased being dissertation.bib.
%
% Either way, the each entry has a key (or identifier) which can be used
% in the main matter to cite it, e.g., \cite{X}, \cite[Chapter 2}{Y}.

\backmatter

\bibliography{dissertation}

% -----------------------------------------------------------------------------

% The dissertation concludes with a set of (optional) appendicies; these are 
% the same as chapters in a sense, but once signaled as being appendicies via
% the associated macro, LaTeX manages them appropriatly.

\appendix

\chapter{An Example Appendix}
\label{appx:example}

Content which is not central to, but may enhance the dissertation can be 
included in one or more appendices; examples include, but are not limited
to

\begin{itemize}
\item lengthy mathematical proofs, numerical or graphical results which 
      are summarised in the main body,
\item sample or example calculations, 
      and
\item results of user studies or questionnaires.
\end{itemize}

\noindent
Note that in line with most research conferences, the marking panel is not
obliged to read such appendices.

% =============================================================================

\end{document}
