% The document class supplies options to control rendering of some standard
% features in the result.  The goal is for uniform style, so some attention 
% to detail is *vital* with all fields.  Each field (i.e., text inside the
% curly braces below, so the MEng text inside {MEng} for instance) should 
% take into account the following:
%
% - author name       should be formatted as "FirstName LastName"
%   (not "Initial LastName" for example),
% - supervisor name   should be formatted as "Title FirstName LastName"
%   (where Title is "Dr." or "Prof." for example),
% - degree programme  should be "BSc", "MEng", "MSci", "MSc" or "PhD",
% - dissertation title should be correctly capitalised (plus you can have
%   an optional sub-title if appropriate, or leave this field blank),
% - dissertation type should be formatted as one of the following:
%   * for the MEng degree programme either "enterprise" or "research" to
%     reflect the stream,
%   * for the MSc  degree programme "$X/Y/Z$" for a project deemed to be
%     X%, Y% and Z% of type I, II and III.
% - year              should be formatted as a 4-digit year of submission
%   (so 2014 rather than the accademic year, say 2013/14 say).

\documentclass[ % the name of the author
author={Dillon Keith Diep},
% the name of the supervisor
supervisor={Dr. Carl Henrik Ek},
% the degree programme
degree={MEng},
% the dissertation    title (which cannot be blank)
title={ART-CG Hair:},
% the dissertation subtitle (which can    be blank)
subtitle={Assisted Real-time Content Generation of Stylised Virtual Hair},
% the dissertation     type
type={Research},
% the year of submission
year={2017} ]{dissertation}
\begin{document}
% =============================================================================

% This section simply introduces the structural guidelines.  It can clearly
% be deleted (or commented out) if you use the file as a template for your
% own dissertation: everything following it is in the correct order to use 
% as is.

% =============================================================================

% This macro creates the standard UoB title page by using information drawn
% from the document class (meaning it is vital you select the correct degree 
% title and so on).

\maketitle

% After the title page (which is a special case in that it is not numbered)
% comes the front matter or preliminaries; this macro signals the start of
% such content, meaning the pages are numbered with Roman numerals.

\frontmatter

% This macro creates the standard UoB declaration; on the printed hard-copy,
% this must be physically signed by the author in the space indicated.

\makedecl

% LaTeX automatically generates a table of contents, plus associated lists 
% of figures, tables and algorithms.  The former is a compulsory part of the
% dissertation, but if you do not require the latter they can be suppressed
% by simply commenting out the associated macro.

\tableofcontents
\listoffigures
\listoftables
\listofalgorithms
\lstlistoflistings

% The following sections are part of the front matter, but are not generated
% automatically by LaTeX; the use of \chapter* means they are not numbered.

% -----------------------------------------------------------------------------

\chapter*{Executive Summary}
Production of 3D virtual worlds is a time-consuming and costly process that also demand expert knowledge. 3D assets encompass a vast range of applications, ranging from simulations and research to contributing towards the functioning of many industries. Our proposition of probabilistic non-linear dimension reduction consolidates observed variables to provide a parsimonious collection of latent variables, drastically diminishing the complexity of the production pipeline through regression. Simplifying a procedure within the pipeline yield increased throughput and offer an opportunity for rapid prototyping. A non-linear approach minimises loss of information; thus high dimensional data can be reduced effectively to dimensionality that is sufficiently low. Non-experts may find directing a few descriptive components within a low dimension representation easier than controlling an overwhelming number of attributes present in the original data.

One particular task is the creation of 3D hair geometry for humanoid characters. Creating 3D hair is arduous as hair structure is a complex system containing much interdependence between components. The properties of 3D hair geometry pose significant challenges for machine learning solutions. Machine learning typically uses large data sets for training on problems that often have a concise answer for a given prediction. The application of machine learning to enhance production for creative work is an exciting field that faces novel challenges: artistic products tend to have small sets of data available, and evaluation of quality is subjective. Given the same input, acceptable solutions can vary significantly. The outlined peculiarities of applying machine learning to 3D mesh data establish a unique field of problems to investigate.

Existing tools for 3D modelling have remained mostly static in the paradigm of approach over the past several decades. Automation through methods such as procedural generation can produce content faster. However, traditional synthesis and automated solutions are defined to produce a specific class of output through established patterns and cannot adapt to new models without reimplementation. The research hypothesis of this study is that of applying non-linear probabilistic dimensionality reduction improves the efficacy of creative content production for exceptionally high dimensional data such as complex 3D hair geometry on virtual humanoids.

\begin{quote}
	Deliverables:
	\begin{itemize}
		\item Formulated a generative model for 3D humanoid hair structure.
		\item Standardised mesh data for training through repairing raw input mesh data then feature extraction to estimate inputs of our generative model.
		\item Learned a low-dimensional latent-space of high-dimensional hair structure data with a probabilistic latent variable model.
		\item Investigated the performance of various kernels and their impact on the latent variable model.
		\item Implemented a demonstrative add-on package for a 3D production program, Blender.
		\begin{itemize}
			\item Generating guiding hair strands from the output of our regression model.
			\item Developed with a small training set that is practical for content creators.
			\item Achieving real-time performance, matching state of the art non-learning tools.
			\item Including functionality to records user activity for analysis.
		\end{itemize}
		\item Evaluation of data collected from survey and experiments.
	\end{itemize}
\end{quote}

% -----------------------------------------------------------------------------

\chapter*{Supporting Technologies}

\begin{quote}
	\noindent
	\begin{itemize}
		\item Python with standard libraries for implementation.
		\item GPy library to implement the regression model.
		\item NumPy and SciPy for mathematical operations
		\item NetworkX for processing meshes as graphs.
		\item Pillow, Matplotlib, and Plotly for plotting images and graphs.
		\item Pickle for serialising objects as files.
		\item Blender API for mesh processing and interfacing with the Blender 3D program for demonstration of our implementation.
	\end{itemize}
\end{quote}

% -----------------------------------------------------------------------------

\chapter*{Notation and Acronyms}

\begin{quote}
	\noindent
	\begin{tabular}{lcl}
		$x$                 &:	& Scalar value\\
		$\bm{x}$            &:	& Column vector\\
		$\bm{x}^T$			&:	& Row vector, superscript T denotes transpose\\
		$x_i$               &:  & the $i$-th value of a vector $\bm{x}$\\
		$\bm{X}$			&:	& Matrix\\
		$\bm{I}$ 			&:	& Identity matrix\\
		$\Re$				&:	& Set of real numbers\\
		$|\bm{X}|$			&:	& Determinant of matrix $\bm{X}$\\	
		$\mathcal{N}\sim(\mu, \sigma^2)$ 			&:	& Gaussian (normal) distribution with mean $\mu$ and variance $\sigma^2$.\\
		ART-CG				&:	& Assisted Real-time Content Generation\\
		MAP					&:	& Maximum A Posteriori\\
		PCA		 			&:	& Principal Component Analysis\\
		GP-LVM	 			&:	& Gaussian Process Latent Variable Model\\
	\end{tabular}
\end{quote}

% -----------------------------------------------------------------------------

\chapter*{Acknowledgements}

%I would like to thank myself for completing this dissertation. 

\noindent
%However, it would not have been possible if not for the advice and guidance of my supervisor, Dr. Carl Henrik Ek, as well as the support of my friends and family. So please, allow me to thank you all with utmost sincerity.

% =============================================================================

% After the front matter comes a number of chapters; under each chapter,
% sections, subsections and even subsubsections are permissible.  The
% pages in this part are numbered with Arabic numerals.  Note that:
%
% - A reference point can be marked using \label{XXX}, and then later
%   referred to via \ref{XXX}; for example Chapter\ref{chap:context}.
% - The chapters are presented here in one file; this can become hard
%   to manage.  An alternative is to save the content in seprate files
%   the use \input{XXX} to import it, which acts like the #include
%   directive in C.

\mainmatter

% -----------------------------------------------------------------------------

\chapter{Contextual Background}
\label{chap:context}
This dissertation explores assisted real-time content generation through training regression models using unsupervised machine learning for the production of 3D hair geometry.
The purpose of a 3D object is to describe a surface. Typically we define geometric surfaces using a combination of many primitive components. The craft of 3D content is a meticulous procedure as content creators are required to work with enormously high dimensional observed data.
The objective of this project is to demonstrate the potential of probabilistic non-linear dimensionality reduction on creative content production by presenting a framework that improves the efficacy of producing 3D hair geometry. 
In this chapter, we will introduce complications of producing 3D hair geometry with traditional methods and past research on machine learning applied to creative fields. We will also outline the motivation and significance for research, including challenges and central objectives of this study.

\section{Production of 3D Content}
There are numerous representations of 3D objects in computer graphics. One way to obtain 3D geometry data is to sample surfaces of the physical world with a 3D scanner. Common representations of sampled geometric data include point clouds, range maps, and voxels (figure \ref{3drepresentations}). Such representations are effective for sampling as they reconstruct a surface from allocating many simple components.

\begin{figure}[!h]
	\centering
	\caption{3D representations. A point cloud representation is a collection of points (vertices) that describe surface geometry. Range images map pixels of a depth image to a set of points in the scene. Voxels are unit cubes, corresponding to the concept of pixels, a collection of voxels describe an object volumetrically. Source: \href{http://pointclouds.org/}{Point Cloud Library (PCL)}}
	\includegraphics[scale=0.15]{images/pointCloud}
	\includegraphics[scale=0.2]{images/rangeImage}
	\includegraphics[scale=0.125]{images/octreeVoxel}\\
	\label{3drepresentations}
\end{figure}

Representations have advantages and disadvantages depending on the use case. It is possible to convert between representations; however, converting between representations may incur data loss. Precise representations are often mathematical models such as \textit{NURBS} (Non-Uniform Rational Basis Spline) - frequently applied in the \textit{CAD} (Computer-Aided Design) industry. Mathematical models are precise as they do not suffer from floating-point inaccuracy that is present in many representations. The most widely applied representation for \textit{CGI} (Computer-Generated Imagery) is \textit{polygonal meshes}. In a production environment, it is preferred to define geometry specifically to requirements of the design as opposed to capturing examples. Polygon meshes are simple to define, yet with established techniques such as UV texturing and normal mapping, are sufficiently expressive for visual purposes. A significant amount of study has been conducted in the field of polygon meshes to improve its versatility. The study of polygonal meshes is deeply rooted in computer graphics.

The elements of a polygonal mesh are \textit{vertices}, \textit{edges}, and \textit{faces}. The \textit{topology} of a mesh concerns with the arrangement of its components, well-organised topology is required to maintain geometric qualities when performing algorithmic operations on a mesh. In practice, professionals create polygon meshes with majority quad-face topology (faces formed from four edges) during production. The rendering pipeline often automatically converts polygon meshes to triangle faces (formed from three edges) as an optimisation process. \textit{Quadrilateral mesh} form what we call \textit{edge loops} which can be used to define the structure of geometry, thus conform better with editing tools and algorithms than \textit{triangle meshes}.

State of the art 3D production software such as AutoDesk Maya, 3DS Max, and Blender are advanced programs with a sophisticated list of features. That said, such programs have extremely convoluted user interfaces, even the most experienced professionals do not recognise each and every tool available. The high learning curve with an overwhelming user experience is not beginner-friendly. In fact, most functionality is only accessible through an API (Application Program Interface) via a scripting language.  Experienced users welcome this complexity as it allows creators to customise their ideal workflow, but it also creates a high barrier to entry for non-experts who wish to possess the capability of such 3D programs.

\subsection{3D Hair Geometry}
On average, a human is born with between 90,000 to 150,000 scalp hair follicles \cite{hairfollicles}. It is computationally very expensive to render and animate physically correct hair, but creative liberties are taken to approximate or stylise 3D hair such that it is both acceptable aesthetically and feasible regarding performance. This study considers modelling of hair geometry, the motion of hair is assumed to be its default resting pose.

In recent years, leading solutions for real-time simulation of realistic hair and fur, such as \textit{NVIDIA HairWorks} and \textit{AMD TressFX} has emerged. These solutions, however, currently have limited adoption in comparison to their traditional counterpart of polygonal hair on real-time applications. It is often the case that texture-mapped polygonal hair is used as a fallback when advanced simulation fails. Realism is not necessarily always desirable, and polygon hair can flexibly represent different art styles. In some cases, a blend of multiple representations is used to balance between cost and quality. 3D hair in cinematography with a large budget can afford to render hair with representations that achieve much higher fidelity for major characters, but would still consider using efficient variants for scenarios such as crowd simulation. Ultimately, we can observe that the representation of virtual hair follows a structure of splines with control points that define the overall organisation of strands and wisp segment. This information will allow transferral between representations to an extent.

There are various ways to construct a polygonal hair mesh. We are concerned with hair meshes formed from multiple planar sub-meshes of hair segments such as the one presented in figure \ref{hairExemplar}. This type of hair mesh can express a variety of styles but result in densely arranged geometry. 
Producing hair mesh data is challenging as content creators manipulate data that is exceptionally high dimensional, this reflects on \textbf{the curse of dimensionality}, a term coined by Bellman \cite{curseofdim} which refers to the phenomena where exhibited complexity of high-dimensional spaces grows exponentially greater than those of low-dimensionality. There are two perspectives where the curse of dimensionality applies in our project. First, from the viewpoint of a 3D content creator, appending data points to geometry on a 3D program becomes increasingly difficult. Tightly positioned components are laborious for creators to find or select, with each action having less impact on the overall surface. The other perspective is that of algorithmic complexity. Machine learning methods scale and generalise better when using a few key features, use of many features is computationally expensive and risks overfitting.

Procedural generation techniques produce output that adheres to rules established by the generative model defined. Generation of terrains and city modelling sometimes employ procedural techniques to take advantage of its systematic nature to mass produce variations in agreement with specified patterns \cite{procedural1}. Fractals and methods such as the Lindenmayer system have been used to create patterns that resemble those observed in nature \cite{lsystem}. Automated techniques such as the ones discussed, however, are seldom used for modelling distinct objects with a specific design. It is an involved process to control the output of procedurally generated content without heavily restricting its capabilities. Automated methods that do not learn cannot adapt to changing demands without reimplementation, motivating for a learning-based solution.

\begin{figure}[!h]
	\centering
	\caption{Polygonal hair representation. Image courtesy of Madina Chionidi, permission for use granted.}
	\includegraphics[scale=0.3]{images/hairExemplar}\\
	\label{hairExemplar}
\end{figure}

\section{Related Research of Machine Learning in Creative Fields}
Three major paradigms divide the task of machine learning:
\begin{itemize}
	\item \textit{Supervised learning} is provided input training examples with desired outputs to learn the mapping of inputs to an output.
	\item \textit{Unsupervised learning} seeks to learn the structure of and relation between input data.
	\item \textit{Reinforcement learning} iteratively improve a pool of solutions by simulating an environment that applies concepts inspired by the theory of evolution.
\end{itemize}
The role that learning methods play in both manufacturing and consumer application continue to grow, however, adoption has been slow for creative fields.  Generally, robust models improve in performance as more reliable data is obtained. Creative production values uniqueness and versatility, properties that cause difficulty in machine learning methods. Varying artistic styles in design complicate feature analysis and ambiguity of correctness is problematic when predicting an output. In machine learning, prediction of continuous variables is a \textbf{regression} problem. 

Style-based inverse kinematics introduced the Scaled Gaussian Process Latent Variable Model to learn the probabilistic distribution of a 3D human posture model \cite{styleik}. Character posing from motion data is represented as a 42-dimensional feature vector that encapsulated joint information of a humanoid body. Learning a model of poses established the relation between joints and identified constraints exhibited in the training data - where unusual postures are given a lower likelihood rating.

A latent doodle space is the use of a low-dimension latent-space that has been applied on simple line drawings \cite{latentdoodle}. The motivation of a latent doodle space is to generating new drawings that are inspired by the input data. There are two key phases to derive a latent doodle space: the first challenge is to identify line strokes within drawings, a latent variable method is then used to learn a latent space.

A study by Campbell and Kautz (2014) presented a framework that learns the latent manifold of existing font styles \cite{fontmanifold}. The process involved universal parametrization of fonts to a polyline representation so that a distance measure is applicable and the generative model can interpolate between styles. Unsupervised learning with the GP-LVM model enabled rapid prototyping and non-experts could create font styles without experience on type design.

Drawing assistance powered by large-scale crowd-sourcing explored the potential of data driven drawing to prompt for correction by achieving an artistic consensus \cite{drawingassistance}. A consensus is found by learning a correction vector field from training drawings. Stroke-correction is applied using the correction vector field to adjust user input dynamically.

Chai et al. (2016) introduced AutoHair, a method for automatic modelling of 3D hair from a portrait image \cite{autohair}. The approach extracts information from images and uses a database of hair meshes to construct a 3D representation of the information conveyed. A hierarchical deep neural network trained on annotated hair images learn to segment hair and estimate growth direction within portraits. Data-driven hair matching and modelling algorithm fit meshes from the database to parameters output by the neural net model to automatically produce 3D hair. The experiment developed a traversable hairstyle space of 50,000 hair models, using training images and 3D exemplars obtained from the internet.

Research regarding creative content often parametrise the input data so that machine learning is applicable. There is no clearly defined solution for a problem in the creative field, effective solutions strive for versatility, employing consensus decision making or offering multiple solutions. To overcome the challenges introduced, dimensionality reduction through unsupervised learning with probabilistic latent variable models such as the Gaussian Process Latent Variable Model (GP-LVM) \cite{gplvm} present an opportunity to learn stylistic properties of design and predict multiple acceptable outputs by analysing the likelihood.

\section{Motivation and Significance}
Virtual hair creation is a necessity for characters of CG movies and video games that are embedded within culture both economically and as entertainment. Specialised artists learn to be proficient with the design of hair, variety of styles, and techniques for creation. In comparison to other forms of surfaces, hair meshes are densely concentrated, containing many data points that are exhausting to edit. Maintaining topology and issues such as overlapping surfaces are problematic among detailed meshes. Experienced artists might search for an existing base mesh that is similar to start on, but it is not always the case that such a base mesh exists - there are also concerns for quality, such as poor topology. Each alteration makes less impact as the geometry becomes more detailed and well-defined. Design and production of 3D geometry remain a slow and delicate process.

Non-linear dimensionality reduction models are attractive candidates to resolve issues imposed by high dimensional data such as 3D polygonal hair geometry. That said, the process of machine learning is more involved than simply inputting training data into a learning model. In practice, data acquired are frequently encoded in complicated structures. Such data is seldom useful in its original form; feature extraction may be required to find an orderly representation that helps facilitate the learning stage. 3D meshes data structures are problematic as it can represent the same surface with different arrangements of components. Employing dimensionality reduction on real-world data such as 3D meshes from content production is a motivational point for this study.

Learning the relation of hair structure allows the potential of discovering new hairstyles.  It can also be used to output base geometry that fits the target output better than existing geometry available. Generative methods could ensure a level of quality, an organised topology that fits established specifications. Practical application of machine-learning based tools enhance the workflow of professional users and improve the experience for non-expert consumers. Such tools integrate into the production environment to improve the efficiency of acquiring initial base geometry and visually compare designs during pre-production. Non-expert users receive the ability to produce 3D geometry without requiring to learn the intrinsics of traditional 3D modelling software. The rise of augmented reality and 3D printing inspires the development of generative tools that are intuitive and simplistic to use. Applications that allow users to create personal content could also integrate machine-learning systems such as probabilistic regression models to prevent inappropriate or undesirable creation from being produced while providing options that surpass existing alternatives. Assisted content generation using machine learning can provide a convenient, non-intrusive and intuitive method for rapidly generating new hair geometry from existing data.

\section{Challenges}
This study faces multiple challenges. Firstly, 3D meshes are difficult to compare. The training data in its raw form will have varying dimensions. We can view meshes as samples of the actual surface. Thus meshes that represent the same object could differ drastically in the number of data points depending on its level of detail. Typical feature extraction methods do not work well on meshes as artistic products are sensitive to data loss - any change could affect the perception of final result drastically.

Another problem encountered is the lack of training data. Typical machine learning solutions use huge data sets in the order of hundreds of thousands for training, but for 3D meshes, the standard size of readily available training data is much smaller. Public repositories of 3D polygonal hair ordinarily contain up to thousands of meshes \cite{tsr}. Studios that store and organise past production may match the extent of public repositories, depending on the size of the company. Private repositories of independent artists will rarely exceed the order of hundreds.

The application of machine learning methods must also account for the subjectivity of evaluating artistic assets. The range of acceptable solutions is ambiguous, likened to how hair styles of characters can change drastically during the design phase.

In a production environment, the time required for a technique to return observable result directly affects throughput. For practical usage of assisted content generation, the technique should be reasonably fast in presenting observable output.

\section{Central Objectives}
\label{chap:context:objectives}
%The aim of this study is ...
\begin{enumerate}
	\item Performing dimensionality reduction on 3D mesh data.
	\item Exploring the application of non-linear dimension reduction for high dimensional data such as 3D hair geometry.
	\item Investigate the use of latent variables for identifying stylistic properties of 3D content.
	\item Demonstrate the use of a non-linear manifold to generate new hairstyles from training data.
	\item Enable an intuitive method for both experienced users and non-experts to easily create 3D hair geometry.
	\item Achieve performance close to real-time for practical use.
\end{enumerate}

% -----------------------------------------------------------------------------

\chapter{Technical Background}
\label{chap:technical}
In the previous chapter we briefly introduced the \textit{curse of dimensionality}. High dimensional data is often counter-intuitive to perceive and process. Unfortunately, it is common for observed data to be in a representation of greater dimensionality than it requires. This gives rise to the notion of \textbf{dimensionality reduction}, a sub-field of machine learning that is motivated find a descriptive low-dimensional representation of data.
This chapter will establish the technical foundation required for understanding the probabilistic non-linear dimensionality reduction model we use for assisted real-time content generation of polygonal hair. Starting with principal component analysis (PCA), we review its probabilistic equivalent that contributes to the derivation of the Gaussian Process Latent Variable Model (GP-LVM). Then, we give a formal definition of polygon meshes from the perspective of graph theory - along with concepts that are useful for mesh processing.

\section{Principal Component Analysis}
In multivariate analysis, principal component analysis (PCA) is a statistical technique used to perform linear dimensionality reduction. It was originally introduced by Pearson \cite{pca1901}, and independently developed by Hotelling \cite{pca1933}, where the standard algebraic derivation of PCA was presented.

Consider the properties that describe hair structure. Observable variables that can be measured include location, orientation, length, and colour. The data collected may indicate that some variables change together, this relation is measured as the covariance. The PCA technique searches for an orthogonal set of principal components that retain maximal variance. A principal component can be viewed as a combination of the observed variables. Should two observed variables strongly covary linearly, then it is plausible to describe the data with a single variable instead. The more linearly observed variables covary, the less information is lost from choosing a smaller set of principal components, thus effectively reducing dimensionality.

Given a set of $n$ observed $d$-dimensional data represented as a design matrix, $\bm{X}=[\bm{x}_1,...,\bm{x}_n]^T$, the $q$ principal components $\bm{w}_j$, $j \in \{1,...,q\}$, are the orthonormal axes with maximal variance retained. The first principal component is a linear function $\bm{\alpha}^T_1\bm{X}$ that retains most variance of $\bm{X}$, where $\bm{\alpha}_1 = [\alpha_{11}, \alpha_{12}, ..., \alpha_{1n}]$ is a vector of $n$ constants such that \cite[p.4]{pca2002}:
$$\bm{\alpha}^T_1\bm{X}=\alpha_{11}\bm{x_1}+\alpha_{12}\bm{x_2}+...+\alpha_{1n}\bm{x_n} = \sum^n_{i=1}a_{1i}\bm{x_i}$$
The following principal components are found by looking for a linear function that is orthogonal to the selected principal components and retain maximum variance.

PCA can be performed by singular value decomposition (SVD) of design matrix $\bm{X}$ \cite[pp.44-46]{pca2002},
$$\bm{X=ULV}^T,$$
where given $r = r(\bm{X})$ denotes the rank of $\bm{X}$, then
$\bm{U} \in \Re^{n \times r}$ is a matrix of orthonormal columns that are the left singular vectors,
$\bm{L} \in \Re^{r \times r}$ is a diagonal matrix of the singular values of $\bm{X}$, and
$\bm{V} \in \Re^{d \times r}$ is a matrix of orthonormal columns that are the right singular vectors.

A limitation of standard PCA is the lack of a probabilistic solution. 
It is very likely for there to be multiple sets of principal components that are equally acceptable. In the case of polygonal hair production, this aspect is more prominent as an artistic environment desires to evaluate various plausible designs. A probabilistic model will enable exploration of other stylistic embeddings for hair structure. One way to formulate a probabilistic model is to introduce the notion of noise among observed variables. It is often the case that we assume the noise to follow a Gaussian distribution. In the remainder of this chapter, the Gaussian distribution will take a central role. Therefore, we will proceed to describe its characteristics in detail.

\section{Multivariate Gaussian Distribution}
The Gaussian (normal) distribution is a reasonable prior assumption for data that is subject to the central limit theorem, which states that as the sample size of a population tends to infinity, the distribution becomes normally distributed \cite[p.78]{bishop}.
A random variable $X$ that is normally distributed with mean $\mu$ and variance $\sigma^2$ is denoted as
$$X\sim\mathcal{N}(\mu, \sigma^2).$$
The Gaussian density for a single variable $y$ is expressed as \cite[p.78]{bishop}:
$$\mathcal{N}(y|\mu, \sigma^2)=\frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{(y-\mu)^2}{2\sigma^2}\right),$$
$$\mathcal{N}(y|\mu,\sigma^2) \equiv p(y|\mu,\sigma^2).$$
For a $d$-dimensional vector $\bm{y}$, the multivariate Gaussian distribution with mean vector $d$-dimensional $\bm{u}$, $d \times d$ covariance matrix $\bm{\Sigma}$, and the determinant of $\bm{\Sigma}$ as $\bm{|\Sigma}|$, is
$$\mathcal{N}(\bm{y}|\bm{\mu}, \bm{\Sigma})=\frac{1}{(2\pi)^{\frac{d}{2}}\sqrt{|\bm{\Sigma}|}}
\exp\left(-\frac{1}{2}(\bm{x}-\bm{\mu})^T\bm{\Sigma}^{-1}(\bm{x}-\bm{\mu})\right).$$
The Gaussian distribution is \textit{closed} under addition (\ref{gd:sum}), scaling (\ref{gd:scale}), and multiplication (\ref{gd:prod}) - all of which yields a result that is also a Gaussian distribution \cite[p.200]{gp}. These notable properties will be useful in later sections as it allows analytical integration of multivariate Gaussian distributions.
\begin{equation} \label{gd:sum}
\sum^n_{i=1}y_i\sim\mathcal{N}(\sum^n_{i=1}\mu_i,\sum^n_{i=1}\sigma^2_i)
\end{equation}

\begin{equation} \label{gd:scale}
wy\sim\mathcal{N}(w\mu,w^2\sigma^2)
\end{equation}

\begin{equation} \label{gd:prod}
\mathcal{N}(\bm{x}|\bm{a},\bm{A})\mathcal{N}(\bm{x}|\bm{b},\bm{B}) = Z^{-1}\mathcal{N}(\bm{x}|\bm{c}, \bm{C}),
\end{equation}
$$Z^{-1}=
\frac{1}{(2\pi)^\frac{d}{2}\sqrt{|\bm{A}+\bm{B}|}}
\exp\left(-\frac{1}{2}(\bm{a}-\bm{b})^T(\bm{A}+\bm{B})^T(\bm{a}-\bm{b})\right),$$
$$\bm{c}=\bm{C}(\bm{A}^{-1}\bm{a}+\bm{B}^{-1}\bm{b}),$$
$$\bm{C} = (\bm{A}^{-1}+\bm{B}^{-1})^{-1}).$$

Let $w\sim\mathcal{N}(\mu_1, \sigma^2_1)$ and $h\sim\mathcal{N}(\mu_2, \sigma^2_2)$ be jointly Gaussian distributed variables, if the variables are independent, then $p(w,h)=p(w)p(h)$.
The joint probability density is thus,
$$p(w,h)=\frac{1}{\sqrt{2\pi\sigma^2_1}\sqrt{2\pi\sigma^2_2}}
\exp\left(-\frac{1}{2}\left(\frac{(w-\mu_1)^2}{\sigma^2_1}+\frac{(h-\mu_2)^2}{\sigma^2_2}\right)\right).$$
In matrix form, the joint probability is
$$p(w,h)=\frac{1}{2\pi\sqrt{\sigma^2_1\sigma^2_2}}\exp
\left(
-\frac{1}{2}
\left(
\left[
\begin{matrix}
w \\
h
\end{matrix}
\right]
-
\left[
\begin{matrix}
\mu_1 \\
\mu_2
\end{matrix}
\right]
\right)^T
\left[
\begin{matrix}
\sigma^2_1  &   0\\
0           &   \sigma^2_2
\end{matrix}
\right]^{-1}
\left(
\left[
\begin{matrix}
w \\
h
\end{matrix}
\right]
-
\left[
\begin{matrix}
\mu_1 \\
\mu_2
\end{matrix}
\right]
\right)
\right).$$
Assuming independence, the joint probability density for a $n$-dimensional vector $\bm{y}$ is expressed as
\begin{equation} \label{jointpd}
p(\bm{y})=\frac{1}{2\pi\sqrt{|\bm{D}|}}\exp\left(-\frac{1}{2}(\bm{y}-\bm{\mu})^T\bm{D}^{-1}(\bm{y}-\bm{\mu})\right),
\end{equation}
where $\bm{D}\in\Re^{n \times n}$ is the diagonal matrix of the variances \cite[p.78]{bishop}.

\section{A Probabilistic Model for PCA}
Tipping and Bishop introduced a probabilistic framework for principal component analysis by constraining the noise distribution of a \textbf{latent variable model} \cite{ppca}.

A \textit{latent variable model} transforms a set of $n$ $d$-dimensional observed variables encoded as a design matrix, $\bm{Y}=[\bm{y}_1,...,\bm{y}_n]^T$, to a set of $n$ $q$-dimensional latent (unobserved) variables, $\bm{X}=[\bm{x}_1,...,\bm{x}_n]^T$. Latent variables are parsimonious, it is generally the case that $q \ll d$, explaining the original data with fewer variables. A notable latent variable model is that of \textbf{factor analysis}, one that assumes linearity in relation of the observed data set.
For each observed data point, $\bm{y}_i \in \bm{Y}$, $1 \leq i \leq n $, there is an associated latent variable $\bm{x_i}$. The original data can be represented in terms of the corresponding latent variable as
\begin{equation} \label{ppca:fa}
\bm{y}_i=\bm{Wx}_i+\bm{\mu}_i+\bm{\epsilon}_i.
\end{equation}
The matrix $\bm{W}$ represents the linear relationship between the latent-space with the data-space. Figure \ref{lvm} shows how observed variables can be obtained from a latent variable model.
The parameter $\bm{\mu}_i$ allows for non-zero mean, and the $\bm{\epsilon}_i$ parameter represents noise within the model. Standard PCA can be viewed as a variant of factor analysis where the noise parameter is not accounted for. The maximum-likelihood estimates of $\bm{W}$ will thus generally not correspond to the principal subspace. PCA reduce dimensionality without explaining the relation between the original data and its principal components. Regression is possible with a model that explains the relation between observed variables and latent variables.

\begin{figure}[!h]
	\centering
	\caption{A latent variable model learns the mapping of input $\bm{x}_i$ to output $\bm{y}_i$ by encoding the relationship in matrix $\bm{W}$, which models the behaviour of the underlying function.}
	\vspace{0.1cm}
	\begin{tikzpicture}
		\node[draw, circle] (X) at (-1, 0) {$\bm{x}_i$};
		\node[draw, circle] (W) at (1, 0) {$\bm{W}$};
		\node[draw, circle] (Y) at (0, -1) {$\bm{y}_i$};
		
		\path [->] (X) edge (Y);
		\path [->] (W) edge (Y);
	\end{tikzpicture}
	\label{lvm}
\end{figure} 

The latent variable model developed by Tipping and Bishop performs principal component analysis by modelling the noise parameter of equation \ref{ppca:fa} as an isotropic, spherical Gaussian distribution
The noise values, $\epsilon_i \in \Re^{d \times 1}$, are sampled from a independent spherical Gaussian distribution, $$\bm{\epsilon}_i\sim\mathcal{N}(\bm{0}, \bm{\beta}^{-1}\bm{I}).$$
The conditional probability distribution of a observed variables $\bm{y}_i$ given input variables $\bm{x}_i$ is thus Gaussian distributed as
$$p(\bm{y}_i|\bm{x}_i)=\mathcal{N}(\bm{W}\bm{x}_i+\bm{\mu}_i,\bm{\beta}^{-1}\bm{I}).$$
The prior of latent variables is assumed to be standard Gaussian with zero mean and unit covariance, defined as $\bm{x}_i\sim\mathcal{N}(\bm{0},\bm{I})$. The marginal distribution for the observed data $\bm{y}_i$ is obtained by integrating out the latent variables.
From equation \ref{jointpd}, an arbitrary rotation matrix $\bm{R}^T$ can be applied to the basis, forming the correlated Gaussian,
$$p(\bm{y}_i)=\frac{1}{2\pi\sqrt{|\bm{D}|}}
\exp\left(-\frac{1}{2}(\bm{R}^T\bm{y}_i-\bm{R}^T\bm{\mu}_i)^T\bm{D}^{-1}(\bm{R}^T\bm{y}_i-\bm{R}^T\bm{\mu}_i)\right).$$
This gives an eigenvalue decomposition of the inverse covariance matrix, and thus the covariance matrix,
$$\bm{C}^{-1}=\bm{RD}^{-1}\bm{R}^T,$$
$$\bm{C}=\bm{RDR}^T.$$
As a consequence, we can derive that given $\bm{x}_i\sim\mathcal{N}(\bm{\mu}_i,\bm{\beta}^{-1})$ and $\bm{y}_i=\bm{Wx}_i$, then the distribution of the observed variables can be denoted as $\bm{y}_i\sim\mathcal{N}(\bm{W\mu}_i,\bm{W\beta}^{-1}\bm{W}^T)$.
Thus, with a prior as standard Gaussian, $\mathcal{N}(\bm{0},\bm{I})$, 
$$\bm{Wx}_i\sim\mathcal{N}(\bm{0},\bm{WW}^T),$$
$$\bm{y}_i\sim\mathcal{N}(\bm{0},\bm{C}),$$
where the observation covariance model is $\bm{C}=\bm{WW}^T\beta^{-1}\bm{I}$, with corresponding log-likelihood \cite{gplvm}
\begin{equation} \label{ppca:loglikelihood}
\mathcal{L}=\frac{n}{2}(d ln(2\pi)+ln|\bm{C}|+tr(\bm{C^{-1}S})),
\end{equation}
$$\bm{S}=\frac{1}{n}\sum^n_{i=1}(\bm{y}_i-\mu)(\bm{y}_i-\mu)^T.$$
We can write the likelihood for a data point as
\begin{equation} \label{ppca:likelihood}
p(\bm{y}_i|\bm{x}_i,\bm{W},\bm{\beta})=\mathcal{N}(\bm{y}_i|\bm{Wx}_i,\bm{\beta}^{-1}\bm{I}).
\end{equation}
Integrating over the latent variables gives the marginal likelihood,
$$p(\bm{y_i|W,\beta})=\int p(\bm{y_i|x_i,W,\beta})p(\bm{x_i})d\bm{x_i}.$$
As the prior of probabilistic PCA is modelled as a standard Gaussian distribution, $p(\bm{x_i})=\mathcal{N}(\bm{x_i|0,I})$,
marginalisation of the integral obtains the marginal likelihood of each data point as
$$p(\bm{y_i|W,\beta})=\mathcal{N}(\bm{y_i|0,WW^T+\beta^{-1}I}).$$
Assuming that the data points are independent, the likelihood of the full data set is the product of each marginal likelihood,
$$p(\bm{Y}|\bm{W},\bm{\beta})=\prod^n_{i=1} p(\bm{y}_i|\bm{W},\bm{\beta}).$$

\subsubsection{The Principal Subspace of PPCA}
Tipping and Bishop showed that all potential solutions for $\bm{W}$, the likelihood (\ref{ppca:loglikelihood}), is of the form \cite{ppca} $$\bm{W}=\bm{U}_q(\bm{K}_q-\sigma^2\bm{I})^\frac{1}{2}\bm{R}.$$
One particular case of interest is when the likelihood is maximised,
\begin{equation} \label{ppca:ml}
\bm{W}_{ML}=\bm{U}_q\bm{LR},
\end{equation}
$$\bm{L}=(\bm{\Lambda}_q-\sigma^2\bm{I})^{\frac{1}{2}}$$
The matrix $\bm{U}_q$ contains the column vectors that are the principal eigenvectors, $\bm{\Lambda}_q=[\lambda_1,...,\lambda_q]$ represents the diagonal matrix of the corresponding eigenvalues, and $\bm{R}$ represent an arbitrary orthogonal rotation matrix. 
Maximising the likelihood of $\bm{W}$ by equation \ref{ppca:ml} on the latent variable model defined by equation \ref{ppca:fa} maps the latent-space to the principal subspace of the observed data. Satisfying $\bm{W}_{ML}$, the latent variable model is effectively equivalent to standard principal component analysis.

\section{Gaussian Process Latent Variable Model}
The PCA formulation by Tipping and Bishop allows a probabilistic model, however, it assumes the relation of data is linear. The Gaussian Process Latent Variable Model (GP-LVM) is a non-linear latent variable model derived from a dual of the probabilistic PCA by replacing the inner product kernel with Gaussian processes (Lawrence 2005) \cite{gplvm}. A non-linear embedding is a more suitable model for capturing high dimensional data such as polygonal hair structure.
	
\subsection{Dual Probabilistic PCA}
The dual probabilistic PCA introduced by Lawrence allow for latent mappings to be non-linearised through the kernel trick. It marginalises the parameters, $\bm{W}$, and optimises with respect to latent variables, $\bm{X}$. This is the dual approach of the standard probabilistic PCA where the parameters are optimised and the latent variables are marginalised.

First, a conjugate prior to the likelihood of probabilistic PCA (\ref{ppca:likelihood}) is taken to be a spherical Gaussian distribution,
$$p(\bm{W})=\prod^d_{i=1}\mathcal{N}(\bm{w}_i|\bm{0,I}).$$
As marginalisation of both $\bm{W}$ and $\bm{X}$ is often intractable in practice, $\bm{W}$ is selected for marginalisation as the conjugate prior is Gaussian distributed, thus, it can be integrated analytically.
The marginalised likelihood of $\bm{W}$ is
$$p(\bm{Y|X},\bm{\beta})=\prod^d_{i=1}p(\bm{y}_{:,i}|\bm{X},\bm{\beta}),$$
The $\bm{y}_{:,i}$ parameter represents the $i^{it}$ column of $\bm{Y}$, where
\begin{equation} \label{gplvm:marginal}
p(\bm{y}_{:,i}|\bm{X},\bm{\beta})=\mathcal{N}(\bm{y}_{:,i}|\bm{0,XX}^T+\bm{\beta}^{-1}\bm{I}).
\end{equation}
The objective function is the log-likelihood
\begin{equation} \label{dppca:loglikelihood}
L=-\frac{dn}{2}ln2\pi-\frac{d}{2}ln|\bm{K}|-\frac{1}{2}tr(\bm{K^{-1}YY}^T),
\end{equation}
$$\bm{K=XX}^T+\bm{\beta}^{-1}I.$$
In the original paper, Lawrence found the gradients of the log-likelihood (\ref{dppca:loglikelihood}) with respect to $\bm{X}$ as
$$\frac{\sigma L}{\sigma \bm{X}}=\bm{K^{-1}YY}^T\bm{K^{-1}X}-d\bm{K^{-1}X}.$$ 
A stationary point where the gradients are zero is given by
$$\frac{1}{d}\bm{YY}^T\bm{K^{-1}X=X}.$$
The values for $\bm{X}$ which maximise the likelihood are given by singular value decomposition of $\bm{X}$,
$$\bm{X=ULV}^T.$$
$\bm{U}$ is a matrix whose orthonormal column vectors are the first eigenvectors of $\bm{YY}^T$. $\bm{L}$ is a diagonal matrix of singular values, whose $i^{th}$ element is $l_i=(\lambda_i-\frac{1}{\beta})^{-\frac{1}{2}}$, where $\lambda_i$ is the eigenvalue associated with the $i^{th}$ eigenvector $\frac{1}{d}\bm{YY}^T$. $\bm{V}$ is an arbitrary rotation matrix. Lawrence showed that the eigenvalue problem developed here is equivalent to the eigenvalue problem solved in probabilistic PCA, and thus, dual probabilistic PCA is also effectively equal to standard PCA when the likelihood is maximised.
Dual probabilistic PCA assumes that the output dimensions are linear, independent, and identically distributed. Infringing upon these assumptions derive new probabilistic models.

\subsection{Gaussian Processes}
A Gaussian process (GP) is a non-parametric statistical model that is a distribution over functions. It treats each observed variable as an independent distribution. A prior probability distribution is an assumption of belief before taking into account of evidence. Observing the output of a continuous function provides information regarding its behaviour around that specific point. For a noiseless model, we can be certain that the input must intersect the point at an observed output, as shown in figure \ref{gpplot}. An observation refines our belief to obtain a posterior. A model with noise can \textit{infer} that the mapping is nearby the observation, thus treating each observed variable as an independent distribution allows us to take into account of the noise model for prediction.

\begin{figure}[!h]
	\centering
	\includegraphics[scale=0.5]{images/gpPrior}
	\includegraphics[scale=0.5]{images/gpPosterior}
	\caption{Plot of a Gaussian process prior (left) and posterior (right). The semi-transparent functions represent samples from the likelihood distribution.}
	\label{gpplot}
\end{figure}

Formally, a Gaussian process is an finite collection of random variables that are jointly Gaussian, specified by a mean function $m(\bm{x})$ and covariance function $k(\bm{x},\bm{x'})$ of a real process $f(\bm{x})$ \cite[p.13]{gp},
$$f(\bm{x})\sim\mathcal{GP}(m(\bm{x}), k(\bm{x}, \bm{x'})),$$
where
$$m(\bm{x})=\mathbb{E}[f(\bm{x})],$$
$$k(\bm{x},\bm{x'})=\mathbb{E}[(f(\bm{x})-m(\bm{x}))(f(\bm{x'})-m(\bm{x'}))].$$
The mean function of a Gaussian process is generally assumed to be zero, unless stated otherwise.
Polynomial regression models yield best results when the behaviour of the observed data resembles the selected polynomial function, but determining a suitable function is challenging. As a non-parametric model, Gaussian processes provides a probability distribution over a space of functions that associates a likelihood for each function, presenting the opportunity to sample various functions that encode the behaviour of observed data. Gaussian process regression models can be overfitted, but overfitting a space of functions is more lenient than overfitting a single function.

\subsubsection{Gaussian Processes for Regression}
The observed data $\bm{y}$ is assumed to be modelled by a function $f(\bm{x})$ with input $\bm{x}$ and corrupted by noise, $\epsilon$. Noise interference depends on the problem, a simple relationship is a model with additive noise,
$$f(\bm{x})=\bm{x}^T\bm{w}
\hspace{1cm}
y = f(\bm{x}) + \epsilon.$$
Here, $\bm{w}$ represents the parameters that specify the behaviour of the function. Take $\bm{X}$ as the design matrix of the input values. Suppose that the noise on each observed variable is modelled by an independent Gaussian distribution, $\epsilon\sim\mathcal{N}(0, \sigma^2_n)$, the joint likelihood is the product of marginal likelihoods,
$$p(\bm{y}|\bm{X},\bm{w})=\prod^n_{i=1}p(y_i|\bm{x}_i, \bm{w})=\mathcal{N}(\bm{X}^T\bm{w}, \sigma^2_n\bm{I}).$$

In \textbf{Bayesian reasoning}, we update our prior knowledge with observed evidence to obtain a posterior. This methodology is expressed in \textit{Bayes' Theorem},
$$\text{posterior}=\frac{\text{likelihood}\times\text{prior}}{\text{marginal likelihood}}, \hspace{1cm} p(\bm{w}|\bm{y},\bm{X})=\frac{p(\bm{y}|\bm{X},\bm{w})p(\bm{w})}{p(\bm{y}|\bm{X})}$$
The marginal likelihood is given by
$$p(\bm{y}|\bm{X})=\int p(\bm{y}|\bm{X},\bm{w})p(\bm{w})d\bm{w}.$$
The posterior expresses what we know about the parameters using the likelihood and the prior. We can then use the posterior to make an informed prediction for test inputs.

The dual probabilistic PCA model uses a Gaussian process prior that is corrupted by Gaussian noise \cite{gplvm}, $\epsilon\sim\mathcal{N}(\bm{0},\beta^{-1}\bm{I})$.
The covariance function (kernel) is thus,
\begin{equation} \label{gp:prior}
k(\bm{x}_i,\bm{x}_j)=\bm{x}^T_i\bm{x}_j+\beta^{-1}\delta_{ij}.
\end{equation}
Parameters $\bm{x}_i$ and $\bm{x}_j$ are vectors from the space of inputs to the function and $\sigma_1{ij}$ represents the Knronecker delta, defined by 
$$
\delta_{ij} =
\begin{cases}
1, &         \text{if } i=j,\\
0, &         \text{if } i\neq j.
\end{cases}
$$
Taking inputs from matrix $\bm{X}$ and evaluating the covariance function at each observed variable gives the covariance matrix,
$$\bm{K=XX}^T+\beta^{-1}\bm{I}.$$
The element at $i_{th}$ row and $j_{th}$ column of $\bm{K}$ is given by the prior distribution (\ref{gp:prior}). Thus, the marginal likelihood of dual probabilistic PCA is a product of $d$ independent Gaussian processes. The covariance function of a Gaussian process describes the properties of functions, such as variability. Learning in Gaussian processes is to determine hyperparameters of a covariance function that is suitable for the problem being modelled.

\section{Bayesian Gaussian Process Latent Variable Model}
The Bayesian Gaussian Process Latent Variable Model (Bayesian GP-LVM) \cite{bgplvm} extends GP-LVM. The standard GP-LVM method trains by finding the \textit{maximum a posteriori} (MAP) estimate of $\bm{X}$ and jointly maximizing with respect to the hyperparameters. Bayesian GP-LVM performs variational inference to marginalise the latent variables. This method enables optimisation of the resulting lower bound on the marginal likelihood with respect to the hyperparameters. 

The marginal likelihood of the observed data is obtained by integrating out the latent variables:
$$p(\bm{Y})=\int p(\bm{Y|X})p(\bm{X})d\bm{X}.$$
Computationally, this integration is intractable in practice. A variational distribution, $q(\bm{X})$, can instead be used to approximate the posterior distribution over the latent variables, $p(\bm{X|Y})$.
$$q(\bm{X})=\prod^n_{i=1}\mathcal{N}(\bm{x}_i|\bm{\mu}_i,\bm{S}_i).$$
$\bm{\mu_n}$ and $\bm{S}_i$ are the variational parameters. $\bm{S_i}$ is taken as a diagonal covariance matrix. 
The variational distribution can then be used to obtain a Jensen's lower bound on $\log p(\bm{Y})$:
$$F(q)=\int q(\bm{X})\log \frac{ p(\bm{Y|X})p(\bm{X}) }{ q(\bm{X}) } d\bm{X}$$
$$=\int q(\bm{X})\log p(\bm{Y|X})p(\bm{X})d\bm{X} - \int q(\bm{X})\log\frac{q(\bm{X})}{p(\bm{X})}dX$$
$$=\tilde{F}(q)-KL(q||p).$$
The $KL(q||p)$ term is the negative KL divergence between the variational posterior distribution $q(\bm{X})$ and the prior distribution $p(\bm{X})$ over the latent variables. As the KL divergence is Gaussian, by the closed properties of the Gaussian distribution, we know it is analytically tractable. The problematic term is $\tilde{F}(q)$, which is solved by variational sparse Gaussian process regression \cite{bgplvm}.

A fully marginalised GP-LVM approximated by variational inference establishes a Bayesian perspective that is robust to overfitting, thus applicable to data sets even with missing or uncertain observed data. Now that we have established a foundation for probabilistic latent variable models, the next section will look at the data structure of our training input data, the 3D polygon mesh representation.

\section{The Mesh Data Structure}
Polygon mesh data structure used in contemporary 3D programs vary by implementation. The \textit{winged-edge polyhedron} representation defined by Baumgart in 1972 \cite{wingededge} specified the essential components required to model a polygonal mesh: vertices, edges, and faces. We take advantage of the structural similarity between a mesh and a graph to inspire a formal definition for polygonal mesh geometry.

A \textbf{graph} $G$ can be defined as $G=(V,E)$ where $V$ is a non-empty finite \textbf{vertex set} and $E$ is the \textbf{edge set} \cite[p.8]{graphtheory}. An edge $e$ joins a pair of vertices in the graph together, defined as $e=(v_1, v_2)$ where $\forall(v_1 \land v_2)\in V$.

Let polygon mesh $P = (V, E, F)$, where $V, E, F$ represents the set of vertices, edges, and faces respectively. In practice, polygonal meshes contain more components that influences surface appearance such as texture coordinates and vertex normals, however, the components described are sufficient for geometric processing. We assume vertex set and edge set of a mesh forms an \textbf{undirected simple graph}, where there is at most one edge for each pair of vertices and edges are bidirectional links.

A \textbf{mesh vertex} $v$ is a 3D point of the form
$$\forall (x \land y \land z) \in \Re, v = (x, y, z).$$
The set of vertices is a point cloud representation of the geometry. 

A \textbf{mesh edge} $e$ is an unordered pair that connects two vertices. It is described in the form
$$\forall (v_1 \land v_2) \in V, e = \{v_1, v_2\}.$$
Vertices connected by edges form a wireframe of the geometry. 

An \textbf{n-gon face} is formed from an arbitrary number of vertices 
$$\forall (v_1 \land v_2 \land ... \land v_n) \in V, f_n = (v_1, v_2,..., v_n),$$ 
however, we are only concerned with tri-faces $f_3$ when rendering, and quad-faces $f_4$ during content creation, where
$$\forall (v_1 \land v_2 \land v_3) \in V, f_3 = (v_1, v_2, v_3),$$
$$\forall (v_1 \land v_2 \land v_3 \land v_4) \in V, f_4 = (v_1, v_2, v_3, v_4).$$
The face component describes the geometric surface of an object.

\subsection{Topology}
The term topology in mesh modelling refers to the organisation of mesh components \cite[p.91]{blenderstudio}. A mesh is a sample of the true object it represents, the construction of meshes can vary significantly even if they represent the same object. An intuitive explanation is to consider a high level of detail mesh with a low level of detail mesh of the same object - while visually the two might be evidently similar, this relationship is much harder to convey when observing the raw mesh data. The placement of vertices along with the arrangement of edges and faces determine how well the mesh will adapt to operations such as deforming and modifying algorithms, as well as how much computational resource it will consume.

\subsubsection{Poles}
A pole is a vertex that does not have exactly four edges connected to it \cite[p.92]{blenderstudio}. Meshes in production are organised using quad-faces, thus the majority of vertices are expected to have four edges. Poles with two or three edges are often necessary, however, poorly placed poles can be problematic when applying deformation.

\subsubsection{Edge Loops}
An edge loop is a set of connected non-forking edges that traverses the polygon geometry until it either forms an unbroken ring (cyclic edge loop), or ends at a pole \cite[p.93]{blenderstudio}. Typically, edge loops are applied to mimic organic structures for maintaining a clean topology \cite[pp.10-12]{edgeloops}. Effective use of edge loops enable the mesh to deform smoothly during editing and animation. Edge loops can be extracted from the mesh edges, providing structural information of the geometry.

\section{Summary of Technical Background}
Our motivation for dimensionality reduction on 3D geometry with a latent variable model aspires to simplify the production process by explaining the underlying behaviour of observed mesh data. The learning models discussed in this chapter adopt a probabilistic framework so that artists can retain creative control. The next chapter will examine dimensionality reduction for the mesh data structure with the learning models introduced.
% -----------------------------------------------------------------------------

\chapter{Project Execution}
\label{chap:execution}

This chapter covers the process of assisted real-time content generation for 3D hair geometry by applying non-linear dimensionality reduction to obtain a \textit{regression model} and a \textit{latent manifold} for selecting output. The project dedicated a significant portion to the processing of input polygonal hair mesh data so that the non-linear dimensionality reduction can be effectively applied. First, we outline the process of acquiring appropriate 3D mesh input data and challenges imposed by using polygonal meshes as training data. Then, we discuss why it is hard to apply machine learning methods on mesh data structure and presenting a solution to address this issue by sampling generative inputs from the mesh. The training process gives a regression model which can predict output data that represents hair structure from latent input variables. Reducing the dimensionality of generative inputs to two latent variables allow us to plot the likelihood distribution of the latent manifold as a 2D image. We present a demonstrative implementation as an add-on for Blender 3D, where the 2D latent manifold serves as a method of selecting latent variables as input for the regression model to create hair geometry. Figure \ref{latentAddon} displays an image capture of the demonstrative add-on in use.

\begin{figure}[!h]
	\centering
	\includegraphics[scale=0.3]{images/latentHairAddon}
	\caption{A screen capture of the assisted real-time content generation add-on for Blender. It loads the regression model and latent manifold image obtained from the training process.}
	\label{latentAddon}
\end{figure}

\section{Training Data Set}
The machine learning process begins with training data acquisition. Suitable 3D hair geometry is scarce when compared to other mediums such as images. Courtesy of Electronic Arts there exists an active community that produces free for non-commercial use custom content for their gaming software - which includes polygonal hair \cite{tsr}. Files acquired are encoded in the \textit{Package} format, developed for a video game series, \textit{The Sims}. Open-source community software \textit{s4pe} is used to read the \textit{Package} file and extract geometry in \textit{Simgeom} format \cite{s4pe}. The \textit{Simgeom} format is then converted to \textit{OBJ} format using yet another open-source program \textit{S4CASTools} \cite{s4cas}. Figure \ref{inputHairs} present four examples of the training mesh acquired. The geometry extracted are already standardised in scale and orientation.

\begin{figure}[!h]
	\centering
	\includegraphics[scale=2]{images/inputHair1}
	\includegraphics[scale=2]{images/inputHair2}
	\includegraphics[scale=2]{images/inputHair3}
	\includegraphics[scale=2]{images/inputHair4}
	\caption{Examples of polygon hair mesh training data.}
	\label{inputHairs}
\end{figure}

\subsection{Retopologising Training Data}
\textit{Retopology} is the act of refactoring the topology of a surface into a different arrangement of mesh components without altering the surface represented. As mentioned, it is often the case for 3D mesh topology to be organised by quadrilateral faces during production. Rendering pipelines convert quad meshes to triangles as an optimisation process. \textit{Simgeom} data are triangulated meshes; reconstruction is performed on all input data to convert the geometry from triangulated meshes to have quadrilateral topology. It is possible to partially automate this process through functionality offered by the Blender API. A script to batch process conversion of multiple meshes executes the following steps in Blender:
\begin{enumerate}
	\item First, a new blank scene is created.
	\item Import a mesh object from the directory that contains the training data set.
	\item Select the imported mesh and enter edit context mode which enables operations for manipulating objects.
	\item Select all vertices of the mesh and apply the \textit{tri-to-quad} conversion operation.
	\item Enter object context mode to export the mesh.
	\item Delete the mesh from the scene
	\item Repeat from step 2 with the next mesh in the training directory until processing of all meshes.
\end{enumerate}

Existing algorithmic solutions for converting a triangle mesh to a quad mesh are imperfect. Remaining triangular faces are required to be removed manually after the procedure. The conversion process, expressed in figure \ref{fig:triToQuad}, alters the geometry very marginally - but preserves the representation of hair structure. The quad-faced meshes are used for feature extraction and are assumed to be correct in the context that it is representative of a valid hairstyle. 

\begin{figure}[!h]
	\centering
	\includegraphics[scale=2]{images/triToQuad1}
	\includegraphics[scale=2]{images/triToQuad2}
	\includegraphics[scale=2]{images/triToQuad3}
	\caption{The retopology process. Triangular faces are highlighted using a selection procedure for all faces that are made of exactly three edges. To the left is an original mesh acquired, the middle shows a selection of remaining triangles after the automatic tri-to-quad conversion, and the right displays a retopologised mesh of entirely quads.}
	\label{fig:triToQuad}
\end{figure}

Sampling four input mesh from the training set (those displayed in figure \ref{inputHairs}), on average there are around $2 \times 10^{2}$ sub-meshes, $1 \times 10^{4}$ vertices and  $3 \times 10^{4}$ edges per mesh. To retopologise the remaining triangles as quads manually, the hair mesh is separated into individual segments. Toggling the visibility of layering sub-mesh surfaces hides obstructing vertex, edge, and face components. Each sub-mesh must be inspected to find remaining triangular polygons. There are many ways one can modify the topology of a mesh; we assume that basic methods such as utilising the knife tool or dissolve tool of Blender takes a non-expert order of seconds per sub-mesh. The time for manually retopologising a partially converted mesh with the complexity of our training data takes order of hours each. Unfortunately, this meant that it is extremely time consuming to prepare meshes defined by the \textit{Simgeom} format, thus for regression models with larger training sets we settled with including partial examples.

\section{Generative Model of Hair}
To utilise machine learning methods for production of 3D hair geometry,  we develop a generative model of 3D hair structure. Mesh data is tricky to compare as topology and fidelity alter both structure of the data encoded and dimensionality. To successfully learn the relation of mesh data, we must first standardise the 3D representation by approximating the mesh to sample generative input of a similar representation possible by a model. Training learns the relation of observed generative inputs obtained from approximating the mesh data.

Wang et al. (2009) parametrised a 3D scalp space using spherical projection \cite{examplebasedhair}. This space approximated the surface of a scalp as a hemisphere with the centre denoting the origin. We apply a similar approach with a spherical coordinate system to determine the surface of a scalp.

The spherical polar coordinate system (figure \ref{fig:sphereCoord}) specifies a point from the origin using three parameters:  $(r,\theta,\phi)$ \cite[pp.123-126]{sphericalcoords}. The \textbf{radial distance}, $r$, represents the distance from the origin. We also measure two angles: a \textbf{polar angle}, $\theta$, from the positive $z$-axis (zenith direction) and an \textbf{azimuth angle}, $\phi$, is measured from the $xy$-plane that is orthogonal to the zenith.

\begin{figure}[!h]
	\centering
	\includegraphics[scale=0.25]{images/sphereCoords}\\
	\caption{The 3D spherical coordinate system.
	Source: \href{https://commons.wikimedia.org/wiki/File:3D_Spherical.svg}{https://commons.wikimedia.org/wiki/File:3D\_Spherical.svg}, Public Domain.}
	\label{fig:sphereCoord}
\end{figure}

Uniformly placing points at a fixed radial distance and constant intervals within constrained angles give a collection of points that represent hair roots of the scalp. Figure \ref{fig:roots} visualise the hair roots placed by one particular arrangement. Increasing the number of hair roots allows sampling of higher fidelity. However, this also demands greater computational resources.

\begin{figure}[!h]
	\centering
	\caption{Sphere meshes visualise root positions. The angle ranges specify coverage area of the scalp, while the interval of placements determines resolution. Fitting a sphere to the reference head mesh approximates the radial distance and origin.}
	\includegraphics[scale=0.5]{images/spherePoints}\\
	\label{fig:roots}
\end{figure}

A total of 342 polylines in our generative model is used to describe hair structure. We construct each polyline from 10 points in a 3D space. The resulting feature vector is 10260 dimensions long. An example is a plot in figure \ref{fig:genModel}.

\begin{figure}[!h]
	\centering
	\includegraphics[scale=3]{images/generativeModel}\\
	\caption{A possible configuration of 10260 data points in the generative model}
	\label{fig:genModel}
\end{figure}

\section{Sampling Generative Inputs from Input Data}

Approximation begins by parsing the geometric data of mesh files into a graph data structure of nodes and edges. The axes of OBJ files are misaligned from the axes of Blender, to align the coordinate system, swap the Y and Z axis, then negate the Y axis. OBJ is a human-readable format that declares a mesh element per line. The elements we are interested in are:
\begin{itemize}
	\item geometric vertices, specified by a line that starts with the "v" character, followed by floating values that represent the x, y, z of the vertex position.
	\item face elements, specified by a line that starts with the "f" character, followed by a list of vertex indices that correspond to a vertex line.
\end{itemize}
Variations of the format may exist by the implementation, but we only require the essentials identified for our purpose. Algorithm \ref{objparse} parses \textit{OBJ} files to retrieve a mesh graph and its corresponding vertex dictionary.

\begin{algorithm}[!h]
	\caption{Parsing OBJ format}
	\algrule
	\textbf{Input}: mesh data file\;
	\textbf{Output}: vertex dictionary of 3D points, mesh graph\;
	\algrule
	\textbf{initialise} vertex dictionary, graph\;
	\For{line \textbf{in} file}{
		\If{line starts with "v "}{
			parse vertex data\;
			add new vertex to vertex dictionary\;
			add new vertex to graph\;
		} \ElseIf{line starts with "f "}{
			parse face data\;
			extract edges from new face\;
			add edges to graph\;
		}
	}
	\Return vertex dictionary, graph
	\label{objparse}
\end{algorithm}

\subsection{Edge Loop Extraction}
The versatile structure of polygon meshes allows it to be an expressive representation. However, this flexibility can cause ambiguity when analysing geometric structure. We assume that the mesh only contains sub-meshes of hair segments that have grid-like topology. Our assumption allows us to extract edge loops that describe the structure of hair segments. In the technical documentation of Blender, it describes an algorithm for edge loop selection. \cite{blenderedgeloop}:
\begin{enumerate}
\item Given a starting edge, only continue searching adjacent edges if the candidates connect to exactly three other neighbours, as any other value would indicate either the border of a mesh or encountering a pole vertex.
\item Completing a cyclic edge loop ends the selection process.
\item Discard adjacent edges that share a face with the current one from consideration.
\end{enumerate}
We devise an edge loop extraction algorithm that achieves the properties specified, illustrated in figure \ref{edgeLoopFig}.

\begin{figure}[!h]
	\centering
	\caption{The edge loop extraction algorithm. It begins by selecting an edge within the mesh graph. The two vertices of the edge are \textbf{end vertices}. We then proceed to \textit{grow} the edge loop selection. Take the set of \textbf{first-degree neighbours} of the \textit{end vertices}; these nodes are the \textit{candidates} for the edge loop. We remove edges that are part of the current edge loop from this set of first-degree neighbours. We add the first-degree neighbours to a set of face vertices for following iterations. Take the neighbours of the first-degree neighbours as a set of \textbf{second-degree neighbours}, removing its originating end vertex. A candidate node is only accepted to the edge loop if its set neighbours do not intersect with the set of face vertices. Append accepted vertices to the list of end vertices will allow the repeat of this process until there are no end vertices left to grow the selection.}
	\includegraphics[scale=0.35]{images/edgeLoopDiagram}\\
	\label{edgeLoopFig}
\end{figure}

\subsection{Boundary and Root Edge Loops}
Hair structure estimation begins by splitting the mesh into sub-meshes, determined by graph connectivity. For each segment mesh, we are interested in the edge loops that represent hair strands from the scalp roots. To find these edge loops, we must locate the \textit{boundary edge loops} of the mesh and choose one to be the \textit{root edge loop}.

\textit{Corner vertices} are nodes that have exactly two edges, while \textit{border vertices} have three edges. Boundary edge loops are determined by selecting an edge of a corner vertex and growing the edge loop. Any edge of a corner vertex will connect to a border vertex. The selection is specified to stop upon encountering another corner vertex, resulting in an edge loop of the bounds. We discard the vertices of the extracted boundary edge loop. Remove any corner vertices that no longer have any neighbours left in the set of border vertices. Repeat until there are no corner nodes left, thus successfully extracting the boundary of the mesh as a collection edge loops.

\begin{algorithm}[!h]
	\caption{Extracting boundary edge loops}
	\algrule
	\textbf{Input}: vertex dictionary, mesh graph\;
	\textbf{Output}: list of boundary edge loops\;
	\algrule
	\textbf{initialise} corner vertices, border vertices, list of boundary edge loops\;
	\While{number of corner vertices \textbf{in} mesh \textbf{greater than} 0}{
		pick a corner vertex\;
		\For{vertex \textbf{in} neighbours of selected corner}{
			\If{vertex \textbf{in} set of border or corner vertices}{
				extract edge loops\;
				add edge loops to boundary edge loops collection\;
				remove vertices of extracted loops from boundary vertices\;
			}
		}
		\For {vertex \textbf{in} corner vertices}{
			\If{all adjacent border edges of corner vertex have been removed}{
				remove corner vertex\;
			}
		}
	}
	\Return boundary edge loops
\end{algorithm}

\begin{figure}[!h]
	\centering
	\caption{A selected root edge loop. Observe that the corner vertices have two edges, and the boundary edges have three. Determining the root edge loop is done by choosing the boundary edge loop that is closest to the scalp surface.}
	\includegraphics[scale=0.5]{images/rootLoop}\\
\end{figure}

The root edge loop is the boundary edge loop that is closest to the surface of the scalp. It serves as a reference for where the hair strands begin. We determine the root loop by finding the average distance of each vertex in a boundary loop. Heuristically, the root border has minimal distance when aligned across the scalp.

There is a significant flaw in this approach as it will not correctly predict roots of hair segments that are represented by multiple attached sub-meshes. A solution to this would be to accept hair that has clear roots first then iteratively join \textit{floating segments} to the end of the closest corresponding \textit{rooted segment} until there are no more, or when the operation is unable to connect any segments further.	

\subsection{Pivotal Strand Polylines}
With the root loop, we can extract a collection of edge loops that represents the \textit{pivotal} (descriptive) hair strands of the segment cluster. Given the neighbours of root vertices, removing the neighbours that are also root vertices will leave edges that represent key strands. When there is one edge, we take the edge loop as a pivotal strand representation. The edge loop will extend until it encounters a pole or ends on the boundary. It is useful to know where the strand starts (from the root), and where it ends. We extract a path from the edge loop graph by continually appending adjacent nodes, starting from the root node. The result forms a polyline, a sequence of connecting lines.

\begin{algorithm}[!h]
	\algrule
	\textbf{Input:} hair mesh\;
	\textbf{Output:} collection of pivotal polylines\;
	\algrule
	\textbf{initialise}\;
	\For{sub-mesh \textbf{in} mesh}{
		find boundary edge loops of sub-mesh\;
		determine root edge loop among boundary edge loops\;
		\For{node \textbf{in} root edge loop}{
			\For{neighbour of node}{		
				\If{node \textbf{not in} root loop}{
					grow pivotal strand edge loop\;
					add pivotal strand to polyline collection\;
				}
			}
		}
	}
	\Return pivotal poly-line collection\;
	\caption{Extracting polyline edge loops}
\end{algorithm}

A \textbf{repair operator} processes the strand polylines to improve the approximation. First, the starting point of floating polylines is attached to the nearest end of a rooted polyline, only if there exists one within a specified vicinity. We discard the remaining floating polylines that are not attached. Secondly, removing polylines that are insignificantly short in length allows for more descriptive polylines to be sampled.

\subsection{Parametrising a Strand Polyline}
The procedure thus far returns a set of strand polylines that have a varying number of nodes. We produce a constant dimension through sampling to acquire a polyline that is applicable for use as the input of our generative model.

First, establish intervals of the distance covered between the points of the original polylines. Now suppose we want to sample the polyline with $n$ points evenly, we can compute the distance where the $i_{th}$ sample point should travel along the polylines as
$$distance=\frac{i \cdot s_l}{n},$$
where $s_l$ is the polyline length.

We determine the indices $j$ and $j+1$ where the sample distance lies on the polyline by comparing the sample distance to the distance of the interval list. The position, $\bm{P}$, of the sample point is
$$\bm{P} = \bm{S} + r\bm{D},$$
where $\bm{S}$ denotes the starting position that is the $j_{th}$ point of the original polyline, plus the unit vector direction to the next $j+1_{th}$ point, $\bm{D}$, multiplied by scalar $r$, the remaining distance to cover from the $j_{th}$ interval.

A potential improvement for parametrising the strand polyline is to concentrate sampling points in areas of high curvature rather than at evenly spaced intervals. Sampling effectively allows the model to be descriptive with fewer parameters, mitigating problems introduced by the curse of dimensionality.

\subsection{Structure Estimation}
Strand estimation extracts an arbitrary number of polylines. The next step is to choose a fixed number of polylines for representing the hair geometry. We introduce a \textbf{selection operator} to associate a root with a polyline such that the combination of polylines is representative of the hair structure. Attributes that make a strand polyline more desirable to a particular root include:
\begin{itemize}
	\item \textit{proximity} - how close the subject strand is to the root position.
	\item \textit{significance} - how definitive a particular strand is, or the information conveyed by the subjected polyline.
	\item \textit{uniqueness} - whether there are other roots that have already captured the information conveyed by the subjected strand.
\end{itemize}
Several selection operators experimented are:
\begin{itemize}
	\item \textit{nearest strand selector} - it naively selects the closest strand of each root position. 
	\item \textit{average strand selector} - picks all strands within vicinity and takes the average representation.
	\item \textit{unique selector} - considers all strands within vicinity and takes the strand that has maximises the minimum distance from any polyline already selected by other roots.
\end{itemize}

\section{Learning a Manifold with Bayesian GP-LVM}
Once we have acquired a method to estimate the hair structure of the input data, we can then learn a latent manifold for generative inputs sampled from hair meshes. 

A perspective that extends GP-LVM is the paradigm of a \textit{kernel method based GP-LVMs} \cite{reviewgplvm}, the choice of a kernel influences performance of the model as it specifies the prior distribution for regression. Which kernel to use depends on the task to be accomplished. Figure \ref{kernels} shows the behaviour of various standard kernels.

\begin{figure}[!h]
	\centering
	\includegraphics[scale=0.4]{images/gpyKernels}\\
	\caption{
	The GPy library offers various standard kernels. Multi-modal kernels can be formed from combining kernels to represent data of complex nature.
	Source: \href{http://gpytest2.readthedocs.io/en/latest/tuto_kernel_overview.html}{GPy Library Documentation: A kernel overview}.}
	\label{kernels}
\end{figure}

The \textit{radial basis function} (RBF) kernel models data smoothly. Generating hair structures with a regression model trained using the RBF kernel interpolate in a gradual manner. The smoothness of the RBF kernel is an attractive property for applying small variations on a hairstyle without destroying its form.

An \textit{exponential kernel} behaves sharply, resembling the training data only very close to the corresponding latent variable on the manifold. Any distance from the latent variables of training data quickly introduces uncertainty, causing the regression model to move towards the mean value. This behaviour can perhaps be useful when setting the mean to a typical default hairstyle for exploring variations, however, a regression model with zero mean quickly produces invalid geometry as surfaces begin to overlap and collapse towards the origin.

Using a \textit{linear kernel} is equivalent to modelling with PPCA. Each latent variable embeds a linear relationship of the original data, thus not as effective for hair geometry that is profoundly non-linear in nature. The majority of the latent manifold in the demonstrative implementation produces nonsensical hair geometry when using the regression model with a linear kernel. Linear relationships, however, are simpler to interpret, thus might have applications as additional controllers for in-depth modelling. Figure \ref{fig:manifold} shows the latent manifolds learned from our training set with RBF, exponential, and linear kernel.

\begin{figure}[!h]
	\centering
	\caption{2D latent manifold plots. From left to right: RBF, exponential, and linear kernels. Brightness represents likelihood of the output.}
	\includegraphics[scale=1]{images/latentPlot_t10}
	\includegraphics[scale=1]{images/latentPlot_exp}
	\includegraphics[scale=1]{images/latentPlot_linear}
	\label{fig:manifold}
\end{figure}

\textit{Hyperparameters} of a kernel specifies its behaviour. A \textit{lengthscale} parameter determines how much the distance of variables influence each other, and the \textit{variance} describes its distribution. We apply \textit{automatic relevance determination} (ARD), which uses the observed data to estimate the hyperparameters of the kernel.

When the regression model is uncertain, it is sensible to bias the prediction towards the mean. While not an optimal solution, transforming the input data to be marginally above the scalp surface is heuristically better than an arbitrary point or the global origin that is usually under the scalp - encouraging uncertain output to move inside the reference head mesh. An inverse transformation is applied to the output after prediction to place that the generated mesh at its intended location. 

\section{Geometry Generation from Regression Output}
Completion of the training process provides a regression model that predicts hair structure from estimated input data using our generative model. A problem encountered was incompatibility of the plotting module in the GPy library with the Python distribution of Blender, where a required standard module \textit{Tk interface} is not present. We resolve this issue by plotting the latent manifold as an image, and serialising the regression model using the \textit{Pickle} module of Python. A Blender add-on implementation (introduced in figure \ref{latentAddon}) can then load the latent manifold image in the UV image editor of the program and through the API, load the serialised regression model to predict latent variables sampled from the manifold.

A user-defined Blender operator class, the \textbf{latent selection modal operator}, is responsible for linking a regression model to Blender. Upon \textit{invocation} for setup, a serialised regression model with its corresponding latent manifold image is loaded. A \textit{modal operator} in Blender listens for events until a signal for \textit{finishing} or \textit{cancellation} is returned. Latent selection uses the modal operator to sample coordinates of a mouse movement event. We transform the sampled mouse coordinates to the latent image coordinate space which is in turn used to obtain the actual latent variables from the regression model. 

\begin{algorithm}[!h]
	\algrule
	\textbf{Input:} program context, mouse event, regression model\;
	\textbf{Output:} latent variables, generative prediction\;
	\algrule
	\textbf{initialise}\;
	\If {latent image exists \textbf{in} program context}
	{
		get mouse position from mouse event\;
		get view position by transforming mouse position from region to 2D view space\;
		get image coordinates from view position\;
		get latent variables from regression model using image coordinates\;
		predict generative inputs with latent variables\;
	}
	\Else {
		handle no latent image error\;
	}
	\Return latent variables, generative prediction
	\caption{Latent Variable Selection}
\end{algorithm}

The output of the regression model is an array of inputs for the generative model. The prediction is restructured to create \textit{guide strands} from the polyline curve data structure of the Blender API as displayed in figure \ref{fig:guidingStrands}. Polygon mesh geometry is created using curve modelling attributes commonly available to 3D software. The \textit{extrude} functionality creates planar geometry along the polylines. As the geometry produced is not textured, we use a generative model that creates more roots and tapering geometry to replicate hair locks.
A \textit{taper object} is a reference curve that defines thickness of the geometry produced along the polyline. A \textit{bevel} on a curve defines the cross-section geometry. The use of bevel and extrusion creates geometry that covers the scalp surface.

\begin{figure}[!h]
	\centering
	\caption{The generative inputs predicted are restructured into guiding strands. Geometry is created in accordance to the hair structure described by the guiding strands.}
	\includegraphics[scale=0.3]{images/guidingStrands}
	\includegraphics[scale=0.3]{images/guidingExtrusion}
	\label{fig:guidingStrands}
\end{figure}

An extension to our generative model would be specifying additional sample attributes such as orientation and width of hair segments; this would enable the regression model to learn such attributes of the input data.

Our demonstrative implementation can be set to track user activity for acquiring behavioural data when using the latent manifold for geometry generation. Data logged include time spent exploring the manifold, mouse movement, and normalised likelihood of selected predictions. Development of an output management procedure assists in organising and writing logged data automatically so that user testing can proceed seamlessly.

\section{Summary of Project Execution}

The motivation for assisted real-time content generation of polygonal hair stems from the complexity of producing high dimensional geometric data.

We collect example meshes for training data. Geometric data representing a surface is a many-to-one relationship. There are many arrangements of geometric properties that could denote the same surface. However, data analysis requires this representation must be standardised beforehand. The proposition of a generative model allows sampling of meshes to acquire standardised generative inputs so machine learning methods can be applied.

Learning is selecting an appropriate kernel and refining the hyperparameters with evidence. We observed the behaviour of various kernels, discussing properties that may be useful. Finally, we incorporate the regression model and latent manifold obtained from learning into a state-of-the-art 3D software and created geometry (figure \ref{fig:outputMeshes}) from the generative information predicted.

\begin{figure}[!h]
	\centering
	\caption{Polygonal hair geometry produced by our non-linear regression model from selecting within a latent manifold.}
	\includegraphics[scale=0.25]{images/outputMesh1}
	\includegraphics[scale=0.25]{images/outputMesh2}
	\includegraphics[scale=0.25]{images/outputMesh3}
	\includegraphics[scale=0.25]{images/outputMesh4}\\
	\includegraphics[scale=0.25]{images/outputMesh5}
	\includegraphics[scale=0.25]{images/outputMesh6}
	\includegraphics[scale=0.25]{images/outputMesh7}
	\includegraphics[scale=0.25]{images/outputMesh8}\\
	\includegraphics[scale=0.25]{images/outputMesh9}
	\includegraphics[scale=0.25]{images/outputMesh10}
	\includegraphics[scale=0.25]{images/outputMesh11}
	\includegraphics[scale=0.25]{images/outputMesh12}
	\label{fig:outputMeshes}
\end{figure}

% -----------------------------------------------------------------------------

\chapter{Critical Evaluation}
\label{chap:evaluation}
Through assisted real-time content generation we aim to improve the efficacy of producing 3D hair geometry. For this reason, we seek to evaluate what our proposition has to contribute towards the process of creating hair geometry. Primary categories of contribution include the usefulness of resulting geometry, the time it takes to produce geometry, user experience and behavioural characteristics of users when modelling with a latent manifold.

This chapter will start by examining two methods devised to obtain feedback regarding the application of 3D modelling with a latent manifold. First, a survey is used to compare results of the generative output to the training set and obtain initial reception to the concept. A sample size of 15 subjects completed this survey. Secondly, we conducted a sequence of experiments that involve users navigating through the latent manifold of our prototype implementation. Activity data was logged to analyse behavioural patterns among subjects. A sample size of 10 completed the experiment. We then proceed to discuss the performance of our demonstrative implementation. The chapter concludes with potential applications of assisted real-time content generation with latent regression models.

\section{Latent Hair Modelling Survey}
The survey starts with a section that queries the name of participants and their experience with producing 3D content. Out of ethical concern, this survey questioned only information necessary for the study, minimising inquiry of personal details. A disclaimer notifies participants how the data collected will be used for research purposes. 60\% of the participants considered themselves non-experts, while the remaining 40\% had some form of experience.

\begin{figure}[!h]
	\centering
	\includegraphics[scale=1.1]{images/surveyMesh1}
	\includegraphics[scale=1.1]{images/surveyMesh2}
	\includegraphics[scale=1.1]{images/surveyMesh3}
	\includegraphics[scale=1.1]{images/surveyMesh4}\\
	\vspace{0.07cm}
	\includegraphics[scale=1.1]{images/surveyMesh5}
	\includegraphics[scale=1.1]{images/surveyMesh6}
	\includegraphics[scale=1.1]{images/surveyMesh7}
	\includegraphics[scale=1.1]{images/surveyMesh8}\\
	\caption{Images presented in the survey, named mesh 1-4 on the first row from left to right, and 5-8 on the second. Training meshes are 1, 4, 5, and 6. Output mesh are 2, 3, 7, and 8.}
	\label{surveyMeshes}
\end{figure}

The first section presents participants with a series of images (displayed in figure \ref{surveyMeshes}). The presented set of images consists of training meshes and generated meshes using the output of our regression model. Participants estimate a likelihood rating between the range 0 (0\%) to 10 (100\%) for how much the geometry resembles hair. 
The latent-space is a low-dimensional representation of the high-dimensional hair geometry learned from training meshes. This section of the survey seeks to evaluate the how effectively have we performed dimensionality reduction on hair geometry by observing whether participants believe there is an evident difference between the low-dimensional representation of hair geometry output generated from a latent-space and original high-dimensional training hair meshes. 

A mixture of training and output mesh is presented independently without notifying the subjects which set a particular example belongs. Comparing results of the two sets can evaluate a relative difference between the training data and output result regardless of how a participant perceives hair resemblance. If the subject believes that the training meshes are a good indicator for hair, then success criteria for our generative model would be to match those likelihood ratings. Likewise, a low absolute rating for the output mesh is acceptable if the ratings of training meshes are also low.

\begin{figure}[!h]
	\centering
	\includegraphics[scale=0.6]{images/meshRating}
	\caption{The mean, standard deviation, and variance comparison of training and output mesh presented on the survey.}
\end{figure}

\begin{table}[!h]
	\centering
	\begin{tabular}{|c|c|c|c|}
		\hline
		Mesh Type 	& Mean		& Variance	& Standard Deviation\\
		\hline
		Training 	& 7.92      & 2.62		& 1.62\\
		\hline
		Output 		& 5.08 		& 6.28 		& 2.51\\
		\hline
	\end{tabular}
	\caption{Mean, standard deviation, and variance of training and output mesh (3 s.f).}
\end{table}

In general, training mesh is consistently rated very highly in likelihood at 7.92 mean with a standard deviation of 1.62. On the other hand, the output mesh rated moderately with a mean of 5.08 but results spread out more with a standard deviation of 2.51. To determine whether the difference of results is reliable, we perform independent samples $t$-test for inferential evaluation. The null hypothesis is that there exists no statistical significance difference between the samples. The $t$ value is computed as
$$\text{t}=\frac{\text{difference between group means}}{\text{variability of groups}}=\frac{\bar{x_1}-\bar{x_2}}{\sqrt{\frac{\sigma^2}{n_1}+\frac{\sigma^2}{n_2}}}$$
We obtain t-value
$$t = \frac{2.84}{\sqrt{\frac{2.62}{60}+\frac{6.28}{60}}}=7.37.$$
The degree of freedom for a sample size of 120 is 118. The test rejects the null hypothesis for a $p$-value of 0.05, where the critical value of a two-tailed test with a high degree of freedom is 1.96. The result of the t-test indicates that there is extreme statistical significance between the two sets. From this result, we can infer that there is an observable difference between the training and output meshes.

The following section of the survey introduces basic concepts of 3D modelling and a video depicting the use of a latent manifold for generating meshes. Participants answer questions regarding their feelings towards the application of a latent manifold for 3D production.

When questioned on the difficulty of generating output with a latent manifold, the mean is 4.67, suggesting that difficulty is moderate. However, there is a skewed distribution of responses. A modal value of 3 tells us that among the sample, most participants found it relatively easy to generate output with a latent manifold.

\begin{figure}[!h]
	\centering
	\caption{Responses for difficulty rating of generating output with a latent manifold, from 1 (easy) to 10 (difficult).}
	\includegraphics[scale=0.7]{images/surveyDifficulty}
	\label{fig:surveyDifficulty}
\end{figure}

%*{\color{red} Calculate quartile percentiles}
Figure \ref{fig:surveyDifficulty} displays the distribution of responses for difficulty rating of using a latent manifold.
Participants welcomed the idea of using a latent manifold for exploring new hairstyles or variations of existing ones. For exploring styles, the mean is 7.53 with a standard deviation of 1.60, while exploring variations have a mean of 7.27 and standard deviation 1.33. In both cases, the high mean value and relatively low variance indicate that participants believe in the potency of discovering new geometry with a latent manifold.

We then ask participants to estimate how much automation a latent manifold could complete for the 3D geometry production pipeline. Planning of this question overlooked that non-experts are unfamiliar with the production pipeline, leading to mixed responses shown in figure \ref{surveyAutomate}. The response resembled a normal distribution with mean, mode and median at 6. The standard deviation is 2.
\begin{figure}[!h]
	\centering
	\caption{Responses for estimation of automating the 3D production pipeline with a latent manifold, from 1 (none) to 10 (all).}
	\includegraphics[scale=0.7]{images/surveyAutomate}
	\label{surveyAutomate}
\end{figure}

An overwhelming portion of participants is willing to try rapid prototyping with a latent manifold. Out of 15 responses on figure \ref{surveyPrototype}, 6 (40.3\%) participants agreed that they would use a latent manifold for rapid prototyping, 8 (53.3\%) chose \textit{'Maybe'}, and only one participant decided not to use a latent manifold. 

\begin{figure}[!h]
	\centering
	\caption{Responses for whether participants would use a latent manifold for rapid prototyping.}
	\includegraphics[scale=0.7]{images/surveyPrototype}
	\label{surveyPrototype}
\end{figure}

The survey results indicate that while the output of our current generative model is notably imperfect when compared to the training data, it is satisfactory as confidence is high on the efficacy of producing 3D content with a latent regression model. Many participants believe a latent manifold is effective for discovering new styles and modifying variations of existing styles. The drawbacks identified include that some users find traversing a non-linear embedded manifold difficult to grasp, thus while it is easy to have an output, controlling the turnout is non-intuitive. That said, most participants would still consider using a latent manifold for rapid prototyping. There is much future research that could yield improved results, to which we will discuss in chapter \ref{chap:conclusion}.

\section{Latent Application Experiment}
The experiment shows participants a series of images from the output of our regression model, displayed in table \ref{experimentmesh}. They are tasked to find the closest corresponding point on the latent manifold that reproduces the geometry presented. An output file with the data displayed on table \ref{outputfile} writes the logged user activity obtained from the tests. This experiment allows us to observe how users react to the latent manifold and discover interesting or common behavioural characteristics.

\begin{table}[!h]
	\centering
	\caption{Experiment Geometry}
	\begin{tabular}{|c|c|c|c|}
		\hline
		Test number	& Image & Regression likelihood & Manifold image position\\
		\hline
		1
					&\raisebox{-.45\height}{\includegraphics[scale=1.5]{images/surveyMesh2}}
							& 0.929					& 12, 24\\
		\hline
		2
					&\raisebox{-.45\height}{\includegraphics[scale=1.5]{images/surveyMesh8}}
							& 0.925					& 45, 19\\
		\hline
		3
					&\raisebox{-.45\height}{\includegraphics[scale=1.5]{images/surveyMesh3}}
							& 0.902					& 93, 10\\
		\hline
	\end{tabular}
	\label{experimentmesh}
\end{table}

\begin{table}[!h]
	\centering
	\caption{Contents of the output data file.}
	\begin{tabular}{|l|l|}
		\hline
		Data field 				& Description\\
		\hline
		Model 					& The name of the regression model loaded\\
		\hline
		Time taken				& Measure of time spent from starting selection to choosing a result (seconds)\\
		\hline
		Path 					& A list of image coordinates sampled by the cursor during latent selection\\
		\hline
		Distance 				& The total distance traversed by the the path (image pixels)\\
		\hline
		Likelihood				& The normalised probability associated by the regression model\\
		\hline
		Latent Variables		& The selected latent variables\\
		\hline
		Generative Inputs Sampled 	& The predicted output of the selected latent variables by the regression model\\
		\hline
	\end{tabular}
	\label{outputfile}
\end{table}

\subsubsection{Behavioural Observation: Familiarising with the manifold}
\begin{table}[!h]
	\centering
	\caption{Results of a participant who located later targets quickly after spending some time to explore the manifold first.}
	\begin{tabular}{|l|c|c|c|}
		\hline
		Experiment & 1 & 2 & 3\\
		\hline
		Path&
		\raisebox{-.45\height}{\includegraphics[scale=0.5]{images/experiment/Andrei_ONE_REPLOT}} &
		\raisebox{-.45\height}{\includegraphics[scale=0.5]{images/experiment/Andrei_TWO_REPLOT}} &
		\raisebox{-.45\height}{\includegraphics[scale=0.5]{images/experiment/Andrei_THREE_REPLOT}}\\
		\hline
		Path Length (px) & 712 & 171 & 92\\
		\hline
		Likelihood & 0.863 & 0.878 & 0.863\\
		\hline
		Likelihood Difference & 0.066 & 0.047 & 0.039\\
		\hline
		Position & 11, 22 & 44, 28 & 94, 12\\
		\hline
		Target Distance (px) & $\sqrt{5}$ & $\sqrt{82}$ & $\sqrt{5}$\\
		\hline
		Error (\%) & 2.2 & 9.1 & 2.2\\
		\hline
		Time taken (s) & 47 & 27 & 10\\
		\hline
	\end{tabular}
	\label{andreiresult}
\end{table}

A portion of users invests in exploring the manifold thoroughly when first introduced to it. Table \ref{andreiresult} displays one such example. The \textbf{green dot} in the path represents the \textbf{starting location}, and a white path gradually dims as it reaches a \textbf{red dot} that represents the \textbf{ending location}. In the first experiment, the path plotted suggests that the participant scanned through the manifold by tracing around the image border and then shifting diagonally across with jittering movement to observe how the manifold affects output.

Once familiarising with a particular manifold, a participant would able to locate their objective quickly. The first experiment on table \ref{andreiresult} spent 47 seconds to travel a distance 712 pixels. In the second experiment, the participant took half the time (57\%) and only travelled a fourth of the distance in comparison to the first (24\%), while still selecting a point that is close to the target within a tenth of the latent manifold image size (9.1\%). Finally, in the last experiment, the participant was even faster - travelling 92 pixels to select a point that is only 2.2\% of the image size away from the actual target in ten seconds.

\subsubsection{Behavioural Observation: Misleading local optima}

Participants that do not explore the manifold risk becoming stranded within local optima. Results in table \ref{mariaresult} is an example of this. The participant did not explore during the first test, which leads to a lengthy search process during the second test - where the participant primarily searched within an area that resembles the target, despite the fact that the actual target is located elsewhere in the manifold. The presence of misleading output is a property that makes the latent manifold unintuitive to use, as it would seem logical for similar meshes to be close together in the manifold, but a non-linear embedding means that this is not always true. It would be interesting to study how participants adapt to misleading local optima over time with a longer sequence of tests, as it would seem sensible for this problem to diminish when users learn the output of the latent manifold.

\begin{table}[!h]
	\centering
	\caption{The results of a participant whom in there second experiment picked a similar output that is not near the target.}
	\begin{tabular}{|l|c|c|c|}
		\hline
		Experiment & 1 & 2 & 3\\
		\hline
		Path&
		\raisebox{-.45\height}{\includegraphics[scale=0.5]{images/experiment/Maria_Marinova_ONE_REPLOT}} &
		\raisebox{-.45\height}{\includegraphics[scale=0.5]{images/experiment/Maria_Marinova_TWO_REPLOT}} &
		\raisebox{-.45\height}{\includegraphics[scale=0.5]{images/experiment/Maria_Marinova_THREE_REPLOT}}\\
		\hline
		Path Length (px) & 193 & 1130 & 219\\
		\hline
		Likelihood & 0.941 & 0.529 & 0.925\\
		\hline
		Likelihood Difference & 0.012 & 0.396 & 0.023\\
		\hline
		Position & 13, 23 & 19, 41 & 93, 15\\
		\hline
		Target Distance (px) & $\sqrt{2}$ & $2\sqrt{290}$ & 5\\
		\hline
		Error (\%) & 1.4 & 34.1 & 0.05\\
		\hline
		Time taken (s) & 20 & 102 & 27\\
		\hline
	\end{tabular}
	\label{mariaresult}
\end{table}

\subsubsection{Behavioural Observation: Explore then exploit}

Among participants who take significantly longer to select a prediction are those that value precision over speed. Table \ref{nickresult} shows results where the path starts with bold movement, but gradually becomes unstable towards the end. The path could suggest that the participant began with an exploratory mindset and shifting to an exploitative strategy once the participant believes they are close to their target. 

\begin{table}[!h]
	\centering
	\caption{The progressively becomes jittery, suggesting that the participant is trying to find the precise value.}
	\begin{tabular}{|l|c|c|c|}
		\hline
		Experiment & 1 & 2 & 3\\
		\hline
		Path&
		\raisebox{-.45\height}{\includegraphics[scale=0.5]{images/experiment/Nick_ONE_REPLOT}} &
		\raisebox{-.45\height}{\includegraphics[scale=0.5]{images/experiment/Nick_TWO_REPLOT}} &
		\raisebox{-.45\height}{\includegraphics[scale=0.5]{images/experiment/Nick_THREE_REPLOT}}\\
		\hline
		Path Length (px) & 698 & 992 & 1279\\
		\hline 
		Likelihood & 0.898 & 0.420 & 0.835\\
		\hline
		Likelihood Difference & 0.031 & 0.505 & 0.067\\
		\hline
		Position & 14, 30 & 21, 49 & 71, 85\\
		\hline
		Target Distance (px) & $2\sqrt{10}$ & $6\sqrt{41}$ & $\sqrt{6109}$\\
		\hline
		Error (\%) & 6.3 & 38.4 & 78.2\\
		\hline
		Time taken (s) & 106 & 123 & 115\\
		\hline
	\end{tabular}
	\label{nickresult}
\end{table}

\section{Functional Performance}
Our demonstrative implementation produced hair geometry that resembles hair structure. According to the samples presented in the survey, the average likelihood rating assigned to output meshes by participants is 5.09 (50.9\%). This value indicates moderate resemblance to hair when compared to input meshes which received an average likelihood rating of 7.92 (79.2\%). Our choice of generative model exhibits several flaws that hinder the performance of the training (figure \ref{fig:genIssues}). Firstly, approximating input data from raw meshes causes loss of information. Hair segments formed from multiple sub-meshes may not be fully captured, along with sub-meshes that do not fit with the assumption of grid-like topology or unusual shapes. The loss of information leads to observable patches of the scalp surface where hair geometry is expected to cover. Constraints for hair geometry, such as not intersecting and not growing inwards into the scalp, are not enforced by the regression model. Without constraints, the regression model may make predictions containing invalid properties where surfaces depict nonsensical structure, especially when uncertainty is high.

\begin{figure}[!h]
	\centering
	\caption{Issues identified in our generative model. The left image shows scalp surface exposure. In the right image, half of the head mesh is disabled to show hair geometry predicted to be inside.}
	\includegraphics[scale=0.4]{images/baldHair}
	\includegraphics[scale=0.4]{images/intersectHair}
	\label{fig:genIssues}
\end{figure}

It is worth noting that the generative model we use has much potential for improvement. Input estimation currently samples polylines from the raw mesh. The generative model can be extended to include width and orientation. Alternatively, established generative models can be used in place as our regression model learns only the generative input values. Moreover, the geometry generation procedure can perform post-prediction processing to improve the output mesh, incorporating traditional modelling techniques into the framework. With the centre of the reference head sphere, we can obtain a direction from the origin to a root. This direction vector could be used to find a perpendicular plane where the geometry can follow to provide more coverage.

A framework that puts our concept of learning-based assisted content generation to use has two main stages: training and predicting. First, we have the training process which involves:
\begin{enumerate}
	\item Data acquisition.
	\item Kernel and hyperparameter selection.
	\item Learning the regression model.
\end{enumerate}
Once a regression model has been trained, it can then be used for production, the regression process that consists of:
\begin{enumerate}
	\item Loading the regression model.
	\item Selecting a latent variable.
	\item Predicting output of selected latent variable.
	\item Generating geometry with predicted output.
\end{enumerate}

The ambiguous nature of creative content and mesh data structure makes training a regression model for 3D content difficult. We have demonstrated how algorithmic solutions can automate the task of preparing input mesh for learning. Content creators that have quadrilateral meshes will not require the retopologising stage, which is necessary for our training set as it contains limiting meshes triangulated for rendering. The unpredictable structure of mesh representation requires making some assumptions regarding the input mesh topology. Despite so, our implementation degrades gracefully with the presence of flawed training data. From edge loops, we determine the structure of the surface.

Training mesh data is scarce. Non-experts are more so susceptible to the lack of training meshes. Learning-based assisted content generation by non-linear dimensionality reduction is intended to be robust with small training sets. Table \ref{trainingbenchmarks} displays benchmark results of times taken for the training process on our implementation. The benchmark result is highly dependent on many factors including input data, generative model, implementation, and the hardware of the machine. The point to take away is that training a regression model that makes a prediction from selecting variables in a non-linear latent manifold is a reasonably fast, completing in only matter of minutes.

\begin{table}[!h]
	\centering
	\begin{tabular}{|c|c|c|c|c|}
		\hline
		Training Size & Generative Inputs &	Approximation time (s)	& Optimisation time (s)	&	Total time (s)\\
		\hline
		\multirow{3}{*}{10}
		& 2700 	& 100	    & 41     	& 141\\
		& 10200 & 164	    & 49		& 213\\
		& 38850 & 433      	& 86      	& 519\\
		\hline
		\multirow{3}{*}{20}
		& 2700 	& 177      	& 17        & 194\\
		& 10200 & 312      	& 38      	& 350\\
		& 38850 & 877		& 90	    & 967\\
		\hline
		\multirow{3}{*}{30}
		& 2700 	& 265      	& 29      	& 304\\
		& 10200 & 507      	& 65      	& 572\\
		& 38850 & 1321      & 159      	& 1480\\
		\hline
	\end{tabular}
	\caption{Benchmarks of the training process were performed on a machine with an 2.6 GHz CPU (Intel i7-6700HQ), GTX 970M graphics card, and 8GB RAM. }
	\label{trainingbenchmarks}
\end{table}

We use a configuration file to determine properties of our generative model and the fidelity of approximation. In our benchmark tests, we alter the intervals of hair roots to change the number of generative input the regression model uses for training.

Results of table \ref{trainingbenchmarks} show that approximating generative inputs is the most time-consuming procedure. The time required for approximating is dependent on the complexity of the generative model and input mesh. Thus the benchmark displays a linear relation between total approximation time and training size. Hair mesh is one of the most complicated geometry, thus, are likely to use more generative inputs than other categories of surfaces. One can maintain a sustainable number of generative inputs by choice of generative model.

Plotting the latent manifold as a 2D image provides a visualisation of latent variables and corresponding likelihood. Large training sets incorporate more example styles, but too many reduce ease of use as exploring the 2D non-linear manifold would interpolate unintuitively. Training sets of this solution will inherently be small in cardinality. Fortunately, the GP-LVM model is effective with small training sets.

A regression model it can be used to predict generative inputs repeatedly. Serialising regression models trained with small sets will result in distributable file sizes. Content creators have the option to collect and share regression models. Refining a GP-LVM regression model is possible by inferring new observations, updating the posterior distribution. The initial time investment required to train a model is worthwhile if the regression model can reduce total time spent over the course of its usage.

Loading a small regression model and an image is only required once per start-up of a session, the computational cost is negligible with contemporary hardware. A 10200-generative input regression model on the same machine that performed the benchmark tests spends around $10^{-3}$ seconds to predict an output, using the output to generate hair geometry takes an about of $10^{-1}$ seconds. As a user moves the cursor across the manifold, the implementation presents roughly ten hair meshes per second. This performance is sufficient for producing visible feedback as users traverse the latent manifold.

\section{Comparison with existing solutions}
AutoHair by Chai et al. (2016) \cite{autohair} delivers hair models that are visually superior in comparison to our demonstrative implementation. AutoHair is a fully automatic procedure; our solution is not intended to replace artists but to assist their creation process. On better hardware than those we use for our benchmarks (quad-core Intel i7 CPU, NVIDIA GTX 970 graphics card, and 32GB of RAM), it takes less than a minute for AutoHair to process an image for hair modelling. The AutoHair implementation produces output reasonably fast, however, our implementation generates geometry at close to real-time. In a production environment, immediate observable feedback is strongly desirable as designs frequently change and creators often want to be directing the progression. 

The AutoHair implementation used a database of 300 exemplary 3D hair models and a neural network trained on 50,000 portrait images. The significantly larger amount of material required for the training process increases the time needed for training. According to the originating paper, preprocessing of portrait images is outsourced and takes around a minute per image. The 3D hair exemplars take less than ten hours, and training the neural network is about eight hours. A trained implementation of AutoHair can help 3D artists create models, provided that there be an input image available for the desired hairstyle. It can also be used to discover hairstyles if a 3D hairstyle space is pre-generated. It is unlikely that content creators will collect the material required for training their personal predictive model. Our solution is more flexible in this aspect, as it expects small training sets. A small training set makes it much faster in performance, while not a fully automatic process, it is suitable for assisted content generation and a strong contender for rapid prototyping. Accepting meshes as input to produce output meshes is also more fitting for a production environment than a portrait image. Our solution can easily adapt to different genres of 3D models by specifying a generative model for the type of surface or choosing one of the many existing generative models developed.

Example-based hair geometry synthesis by Wang et al. (2009) \cite{examplebasedhair} is a hierarchical synthesis framework that creates hair models by the parametrising of hair as a 3D vector field and a 2D arrangement of hair strands. Fully automatic without user intervention. 100,000 strands take less than two minutes on a 3GHz Pentium 4 CPU with 2GB RAM. Texture synthesis applied to the geometry completes more of the production pipeline than our solution. As a synthesis method, visual properties are controlled with parameters before generation, creating novel hairstyles by taking input hairstyle and finding statistically similar spatial arrangements of geometric details. Without learning, the synthesis-based solution requires an example and cannot flexibly model other surfaces that are attainable through learning latent manifolds. We can incorporate synthesis methods in place of our generative model. The use of a 3D vector field and 2D arrangement of hair strands encoded in data images can provide a volumetric field to sample hair geometry from rather than manipulating a fit set of strands.

\section{Applications}
The advent of 3D printing, augmented reality, and virtual reality has resurfaced interest in the production of 3D representations. Content creation extends to consumer applications. Advancements in communication and technology established a culture that shares experiences on social media. An easy method of creating content is a coveted feature.
For example, consider a system in a video game that allows users to customise their avatar appearance. A content customisation system desires to be flexible but may be abused to create inappropriate content if no restrictions are applied. A custom content system is especially problematic if inappropriate content is shared or displayed through an online platform. Our solution can be employed to address this problem training a regression model for parameters of the customisation system and only accept inputs that have an associated likelihood above a certain threshold. Selecting from a latent manifold can update many parameters simultaneously and interpolate between predefined configurations, providing an excellent level of customisation while adhering to defined constraints.

A 3D latent space offers greater freedom of control for embedding data. Producing in a 3D space through VR has been gaining momentum and could be an interesting solution for when a 2D manifold is not sufficient for describing the data.

Our proposition is specifically for hair. However, it would also be applicable for other hair-like structures such as foliage or grass. The framework we propose is extremely versatile, choosing other generative models would allow application of assisted content generation. 3D sculpting often produces vector displacement maps that deform base geometry to a high-resolution surface. Transferring assisted real-time content generation to model organic surfaces is a plausible claim that improves its applicability significantly.

% -----------------------------------------------------------------------------

\chapter{Conclusion}
\label{chap:conclusion}

Assisted real-time content generation by learning a latent manifold of existing polygonal hair meshes seeks to alleviate the complexity of producing hair through dimensionality reduction.
We have demonstrated the implementation of a framework that assists content creators by automatically modelling high-dimensional geometry at close to real-time performance when navigating through a low-dimensional space.

The erratic nature of defining a surface with mesh data makes it unreliable for data analysis without preparation.
Methods that extract meaningful information from input meshes and standardise the data to obtain a fixed-length feature vector for hair structure were applied.
Strategically sampling data from a mesh using edge loops that convey structural information approximate inputs of a generative model, allowing for dimensionality reduction.
Chapter \ref{chap:execution} tackled the objective of performing dimensionality reduction on 3D meshes in section \ref{chap:context:objectives}.
\ref{chap:execution}. According to the results of \ref{chap:evaluation:mesh}, participants can distinguish between the original high-dimensional training mesh and output mesh generated from the low-dimensional latent space. While not perfect, the low-dimensional representation is still considered to be moderately resembling.

An artistic production environment desires freedom for expression. Performing regression through probabilistic latent variable models fulfils this need by offering a space of possible predictions. 
We analysed and compared the behaviour of the RBF, exponential, and linear kernel for a regression model that predicts hair structure.

The regression model predicts values of a hair feature vector. This data is input into the generative model to reconstruct the hair representation. A geometry generation process uses hair structure as a reference for creating the resulting geometry. Our implementation parses the feature vector of polylines descriptors into curve data and creates geometry through extrusion and bevelling function commonly available on state-of-the-art 3D software.

We acquired feedback through an online survey and a series of experiments completed during a face-to-face session. 
Respecting our central objectives outlined in chapter \ref{chap:context}, we managed to perform dimensionality reduction on hair meshes by sampling a feature vector of estimated inputs for a generative model. Results of the survey indicate that despite embedding the training hair meshes into a two-dimensional latent-space, participants found that the geometry generated moderately resembled hair.

The application of non-linear dimension reduction for high dimensional data such as 3D hair geometry was discussed in chapter \ref{chap:evaluation}. Our implementation demonstrated its application towards the 3D production pipeline.

Latent variables as a consolidation of observed variables are not well-defined attributes. Thus they do not identify stylistic properties. That said, they do encode stylistic properties as the survey results suggest that participants find the latent-space useful for exploring new styles and discovering variations of geometry.

As it turns out, there exist issues that cause the latent manifold to in fact be unintuitive to use. While the results recorded in the feedback find most participants considering the latent manifold intuitive to use, a proportion of users voiced that there are moments when it is challenging to understand how movement in the latent manifold affects the output. There is a consensus that it is suitably easy for non-experts to generate output.

The demonstrative implementation achieves performance close to real-time, as our regression model can predict a feature vector for hair structure with around $10^{-3}$ seconds (milliseconds), and generating geometry take around $10^{-1}$ seconds (deciseconds).

\section{Future Work}
A latent variable lacks a clear description as it represents the underlying unobserved attributes of data. Research contributing towards presenting a latent-space in a meaningful manner is a multidisciplinary study that extends visualisation of data with latent variable models and incorporates psychological factors of user experience such as intuitiveness. This field of work would propel the application of non-linear latent variable models for regression of novel output such as content creation and 3D production.

The use of a generative model or synthesising methods is to standardise mesh data. We have demonstrated our framework for the production of 3D hair geometry through a simple generative model to focus more on the entire framework overall. A review of established generative and learning models expand the options available to our framework, allow for the production of other surface types.

Machine learning research that output 3D data frequently use images or scanned 3D representations that do not convey topological information as input. Production 3D data provide descriptive for the appearance of an object, without any obstructed information present in images and include topology that is useful for algorithmic operations. Study of preserving topological data would improve the efficacy of assisted content generation through regression by dimension reduction.

Empirical analysis over the steps of execution within our framework would provide an explanation for the feedback data acquired and provide more information regarding the performance of the system. Collecting data from a larger sample of participants would also improve the reliability of our results.

Assisted real-time content generation by learning a latent manifold of existing polygonal hair meshes is a study engages in the discussion of automated production for creative content in the future by tying together the topic of machine learning and 3D mesh processing with production qualities in mind. 

\section{Potential Improvements}
The performance of our framework is a cumulative result of the: 
\begin{itemize}
	\item the properties of training data acquired.
	\item the capability of chosen generative model.
	\item the effectiveness of the method for sampling generative inputs.
	\item the regression model obtained from training.
	\item the generation of geometry from predicted output.
\end{itemize}

The presence of triangulated faces in training data hinders the accuracy of our regression model as loss of information is incurred during generative input estimation. Access to a training set of quadrilateral meshes would improve sample estimation of our generative model, yielding an output that is more representative of training mesh. Fortunately, it is common practice for production to organise meshes with quadrilateral topology. Experienced content creators are more likely to have the option of testing various training sets, whereas our demonstration only used meshes that were available.

In chapter \ref{chap:execution}, the generative model of our implementation was introduced. The topic of generative models is extensively studied, more sophisticated representations may improve the fidelity of sampled generative inputs. Splines are better representations of curving hair strands as opposed to polylines. Defining attributes such as orientation, width, tapering on the generative model would grant the regression model richer information regarding the training hair meshes. Piuze et al. (2011) \cite{helicoidhair} is a mathematical-based representation for hair and hair-like structures using generalised helicoids. The helicoid model can define patterns such as wisps and waviness. The helicoid representation also allows interpolation between hair within the vicinity.

The prediction interpolates between styles, drastically differing styles give rise to a conflicting substructure of hair geometry. Perhaps it would be more intuitive to use a latent manifold learned from a training set of similar styles. Campbell and Kautz \cite{fontmanifold} learn manifolds of fonts for individual characters as well as joint manifolds. A single manifold to edit font characters independently allow for more freedom of control. One consideration would be to use a joint manifold for defining an overall hair structure and then refine layers or sections with single manifolds on top. Multiple manifolds divide the problem at hand, reducing outcomes that seem bizarre due to interpolating of non-linear embeddings at the cost of a more preparation required and an involved training process.
A method for determining the threshold of acceptable hair would allow us to prune uncertain predictions from the latent manifold to expand areas of interest that could make exploration of the manifold more intuitive. 
As mentioned previously, 3D meshes are delicate and can be invalidated easily from small changes. Thus, a reparation operator or constraints to ensure that the predicted output of the regression models is acceptable would be a topic to explore.

There are many extensions for GP-LVM. Deep Gaussian processes \cite{deepgp} offers a hierarchical belief network is potentially effective for hair structure as it often exhibits layering segments.
The choice of probabilistic non-linear latent variable models is one that directly impacts the result of hair structures predicted and determines the form of the latent manifold which affects ease of use.

The geometry generation process is vital as it determines the final resulting mesh. It is worth noting that externalising predictable attributes from the training process would reduce the dimension of input data handled by the regression model without affecting the resulting mesh much. Delegating attributes to the geometry generation phase alleviates the curse of dimensionality and allows the regression model to focus on important attributes that are desirable to learn.

The application of a latent manifold for production may seem alien to some users, especially among experienced content creators trained to use traditional approaches. A non-linear embedding can cause prediction of output to morph counter-intuitively. One way that makes the latent manifold more understandable is to plot small image of resulting hair in the corresponding area of the latent manifold. The original paper of probabilistic PCA and GP-LVM both make use of this concept to visualise the embedding of the latent-space.

% =============================================================================

% Finally, after the main matter, the back matter is specified.  This is
% typically populated with just the bibliography.  LaTeX deals with these
% in one of two ways, namely
%
% - inline, which roughly means the author specifies entries using the 
%   \bibitem macro and typesets them manually, or
% - using BiBTeX, which means entries are contained in a separate file
%   (which is essentially a databased) then inported; this is the 
%   approach used below, with the databased being dissertation.bib.
%
% Either way, the each entry has a key (or identifier) which can be used
% in the main matter to cite it, e.g., \cite{X}, \cite[Chapter 2}{Y}.

\backmatter

\bibliography{dissertation}

% -----------------------------------------------------------------------------

% The dissertation concludes with a set of (optional) appendicies; these are 
% the same as chapters in a sense, but once signaled as being appendicies via
% the associated macro, LaTeX manages them appropriatly.

\appendix

\chapter{An Example Appendix}
\label{appx:example}

Content which is not central to, but may enhance the dissertation can be 
included in one or more appendices; examples include, but are not limited
to

\begin{itemize}
	\item lengthy mathematical proofs, numerical or graphical results which 
	are summarised in the main body,
	\item sample or example calculations, 
	and
	\item results of user studies or questionnaires.
\end{itemize}

\noindent
Note that in line with most research conferences, the marking panel is not
obliged to read such appendices.

% =============================================================================

\end{document}
